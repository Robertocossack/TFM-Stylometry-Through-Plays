{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_braquets = \"\\[.*?]\"\n",
    "normal_braquets = \"\\(.*?\\)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraphs_cleaner(file,regex): #Two in one function\n",
    "    f = open(file, \"r\")\n",
    "    contents = f.read()         \n",
    "    f.close()     \n",
    "    paragraph_cutter = re.sub('\\n{2,}', '\\n\\n', contents) #Paragraph identifier\n",
    "    paragraphs = paragraph_cutter.split('\\n\\n') #It cuts the paragraphs by the identifier points\n",
    "    clean_result = []\n",
    "    for par in paragraphs:\n",
    "        output = re.sub(\"\\\\n\", \" \", par) # Removes the new-line characters within the paragraphs\n",
    "        clean = re.sub(regex , \"\", output) # Removes the additional text in brackets\n",
    "        clean_result.append(clean)\n",
    "    result = []\n",
    "    for par in clean_result:\n",
    "        output = nltk.word_tokenize(par) #Paragraph tokenizer\n",
    "        result.append(output)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_paragraph_tokenizer(clean_result):    \n",
    "    result = []\n",
    "    for par in clean_result:\n",
    "        output = nltk.word_tokenize(par) #Toqueniza los p√°rrafos\n",
    "        result.append(output)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_name_cutter (character_raw, n): #Where n is the number of the cut position\n",
    "    result = []\n",
    "    for sen in character_raw:\n",
    "        output = sen[n:]\n",
    "        result.append(output)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_joiner(character):\n",
    "    output = []\n",
    "    for par in character:\n",
    "        for word in par:\n",
    "            output.append(word)\n",
    "    one_string = \" \".join(output)\n",
    "    result = nltk.sent_tokenize(one_string)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_cleaner (character_raw, n): #Two in one function\n",
    "    no_name = []\n",
    "    for sen in character_raw:\n",
    "        output = sen[n:]\n",
    "        no_name.append(output)\n",
    "    output = []\n",
    "    for par in no_name:\n",
    "        for word in par:\n",
    "            output.append(word)\n",
    "    one_string = \" \".join(output)\n",
    "    result = nltk.sent_tokenize(one_string)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_cleaner_by_name (character_raw, name1, name2, name3):\n",
    "    for sen in character_raw:\n",
    "        if name1 in sen:\n",
    "            sen.remove(name1)\n",
    "    for sen in character_raw:\n",
    "        if name2 in sen:\n",
    "            sen.remove(name2)\n",
    "    for sen in character_raw:\n",
    "        if name3 in sen:\n",
    "            sen.remove(name3)\n",
    "    output = string_joiner(character_raw)\n",
    "    for sen in  output: #Removes the single points sentences for a best estimation of the lenghth\n",
    "        if sen == '.':\n",
    "            output.remove(sen)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_cutter (character_raw, name1, name2, name3):\n",
    "    for sen in character_raw:\n",
    "        if name1 in sen:\n",
    "            sen.remove(name1)\n",
    "    for sen in character_raw:\n",
    "        if name2 in sen:\n",
    "            sen.remove(name2)\n",
    "    for sen in character_raw:\n",
    "        if name3 in sen:\n",
    "            sen.remove(name3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An Ideal Husband - Oscar Wilde\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_aih = paragraphs_cleaner(\"an_ideal_husband_wilde.txt\", squared_braquets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_aih = token_paragraphs_aih[48:]  #Comienzo de la obra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "caversham_raw = []\n",
    "goring_raw = []\n",
    "chiltern_raw = []\n",
    "ladychiltern_raw = []\n",
    "mabel_raw = []\n",
    "cheveley_raw = []\n",
    "for par in token_paragraphs_aih:\n",
    "    if 'CAVERSHAM' in par:\n",
    "        caversham_raw.append(par)\n",
    "    if 'GORING' in par:\n",
    "        goring_raw.append(par)\n",
    "    if 'ROBERT' in par:\n",
    "        chiltern_raw.append(par)\n",
    "    if  'LADY' and 'CHILTERN' in par and not 'MABEL' in par and not 'SIR' in par:\n",
    "        ladychiltern_raw.append(par)\n",
    "    if 'MABEL' in par:\n",
    "        mabel_raw.append(par)\n",
    "    if 'CHEVELEY' in par:\n",
    "        cheveley_raw.append(par)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "caversham = character_cleaner(caversham_raw, 3)\n",
    "goring = character_cleaner(goring_raw, 3)\n",
    "chiltern = character_cleaner(chiltern_raw, 4)\n",
    "ladychiltern = character_cleaner(ladychiltern_raw, 3)\n",
    "mabel = character_cleaner(mabel_raw, 3)\n",
    "cheveley = character_cleaner(cheveley_raw, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213, 796, 537, 381, 242, 534)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(caversham), len(goring), len(chiltern), len(ladychiltern), len(mabel), len(cheveley)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Woman of No Importance - Oscar Wilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_awoni = paragraphs_cleaner(\"a_woman_of_no_importance_wilde.txt\", squared_braquets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    " token_paragraphs_awoni = token_paragraphs_awoni[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "illingworth_raw = []\n",
    "allonby_raw = []\n",
    "gerald_raw = []\n",
    "mrsarbuthnot_raw = []\n",
    "hunstanton_raw = []\n",
    "hester_raw = []\n",
    "\n",
    "for par in token_paragraphs_awoni:\n",
    "    if 'ILLINGWORTH' in par:\n",
    "        illingworth_raw.append(par)\n",
    "    if 'ALLONBY' in par:\n",
    "        allonby_raw.append(par)\n",
    "    if 'GERALD' in par:\n",
    "        gerald_raw.append(par)\n",
    "    if  'MRS' and 'ARBUTHNOT' in par:\n",
    "        mrsarbuthnot_raw.append(par)\n",
    "    if 'HUNSTANTON' in par:\n",
    "        hunstanton_raw.append(par)\n",
    "    if 'HESTER' in par:\n",
    "        hester_raw.append(par)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "illingworth = character_cleaner(illingworth_raw, 3)\n",
    "allonby = character_cleaner(allonby_raw, 3)\n",
    "gerald = character_cleaner(gerald_raw, 2)\n",
    "mrsarbuthnot = character_cleaner(mrsarbuthnot_raw, 2)\n",
    "hunstanton = character_cleaner(hunstanton_raw, 2)\n",
    "hester = character_cleaner(hester_raw, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(428, 213, 259, 498, 479, 142)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(illingworth), len(allonby), len(gerald), len(mrsarbuthnot), len(hunstanton), len(hester)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Lady Windermer's Fan - Oscar Wilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_lwf = paragraphs_cleaner(\"lady_windermeres_fan_wilde.txt\", squared_braquets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_lwf = token_paragraphs_lwf[47:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lordwindermere_raw = []\n",
    "erlynne_raw = []\n",
    "augustus_raw = []\n",
    "windermere_raw = []\n",
    "darlington_raw = []\n",
    "berwick_raw = []\n",
    "for par in token_paragraphs_lwf:\n",
    "    if 'LORD' and 'WINDERMERE' in par and not 'LADY' in par:\n",
    "        lordwindermere_raw.append(par)\n",
    "    if 'ERLYNNE' in par:\n",
    "        erlynne_raw.append(par)\n",
    "    if 'AUGUSTUS' in par:\n",
    "        augustus_raw.append(par)\n",
    "    if  'LADY' and 'WINDERMERE' in par:\n",
    "        windermere_raw.append(par)\n",
    "    if  'DARLINGTON' in par:\n",
    "        darlington_raw.append(par)\n",
    "    if  'BERWICK' in par:\n",
    "        berwick_raw.append(par)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lordwindermere = character_cleaner(lordwindermere_raw, 3)\n",
    "erlynne = character_cleaner(erlynne_raw, 3)\n",
    "augustus = character_cleaner(augustus_raw, 3)\n",
    "windermere = character_cleaner(windermere_raw, 3)\n",
    "darlington = character_cleaner(darlington_raw,3)\n",
    "berwick = character_cleaner(berwick_raw, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234, 395, 95, 743, 192, 234)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lordwindermere), len(erlynne), len(augustus), len(windermere), len(darlington), len(berwick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Importance of Being Earnest - Oscar Wilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_tiobe = paragraphs_cleaner(\"the_importance_of_being_earnest_wilde.txt\", squared_braquets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_tiobe = token_paragraphs_tiobe[25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "jack_raw = []\n",
    "algernon_raw = []\n",
    "gwendolen_raw = []\n",
    "bracknell_raw = []\n",
    "cecily_raw = []\n",
    "prism_raw = []\n",
    "chasuble_raw = []\n",
    "for par in token_paragraphs_tiobe:\n",
    "    if 'Jack' in par[0:1]:\n",
    "        jack_raw.append(par)\n",
    "    if 'Algernon' in par[0:1]:\n",
    "        algernon_raw.append(par)\n",
    "    if 'Gwendolen' in par[0:1]:\n",
    "        gwendolen_raw.append(par)\n",
    "    if  'Bracknell' in par[0:2]:\n",
    "        bracknell_raw.append(par)\n",
    "    if  'Cecily' in par[0:1]:\n",
    "        cecily_raw.append(par)\n",
    "    if  'Prism' in par[0:2]:\n",
    "        prism_raw.append(par)\n",
    "    if  'Chasuble' in par[0:1]:\n",
    "        chasuble_raw.append(par)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "jack = character_cleaner(jack_raw, 2)\n",
    "algernon = character_cleaner(algernon_raw, 2)\n",
    "gwendolen = character_cleaner(gwendolen_raw, 2)\n",
    "bracknell = character_cleaner(bracknell_raw, 3)\n",
    "cecily = character_cleaner(cecily_raw, 3)\n",
    "prism = character_cleaner(prism_raw, 3)\n",
    "chasuble = character_cleaner(chasuble_raw, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(492, 451, 247, 285, 296, 102, 82)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jack), len(algernon), len(gwendolen), len(bracknell), len(cecily), len(prism), len(chasuble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pygmalion - George Bernard Shaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_p = paragraphs_cleaner(\"pygmalion_shaw.txt\", squared_braquets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_p = token_paragraphs_p[21:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickering_raw = []\n",
    "higgins_raw = []\n",
    "liza_raw = []\n",
    "gentleman_raw = []\n",
    "taker_raw = []\n",
    "girl_raw = []\n",
    "mrshiggins_raw = []\n",
    "doolittle_raw = []\n",
    "for par in token_paragraphs_p:\n",
    "    if 'PICKERING' in par:\n",
    "        pickering_raw.append(par)\n",
    "    if 'HIGGINS' in par and not 'MRS.'in par:\n",
    "        higgins_raw.append(par)\n",
    "    if 'LIZA' in par:\n",
    "        liza_raw.append(par)\n",
    "    if  'GENTLEMAN' in par:\n",
    "        gentleman_raw.append(par)\n",
    "    if  'TAKER' in par:\n",
    "        taker_raw.append(par)\n",
    "    if  'GIRL' in par:\n",
    "        girl_raw.append(par)\n",
    "    if  'MRS.' in par and 'HIGGINS' in par[0:2]:\n",
    "        mrshiggins_raw.append(par)\n",
    "    if  'DOOLITTLE' in par:\n",
    "        doolittle_raw.append(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickering = character_cleaner_by_name(pickering_raw, 'PICKERING', 'xxxxxx', 'xxxxxx')\n",
    "higgins = character_cleaner_by_name(higgins_raw, 'HIGGINS', 'xxxxx', 'xxxxx')\n",
    "liza = character_cleaner_by_name(liza_raw, 'LIZA', 'xxxxx', 'xxxxx')\n",
    "gentelman = character_cleaner_by_name(gentleman_raw, 'THE', 'GENTLEMAN', 'xxxxx')\n",
    "taker = character_cleaner_by_name(taker_raw, 'THE', 'NOTE', 'TAKER')\n",
    "girl = character_cleaner_by_name(girl_raw, 'THE', 'FLOWER', 'GIRL')\n",
    "mrshiggins = character_cleaner_by_name(mrshiggins_raw, 'MRS', 'MRS.', 'HIGGINS')\n",
    "doolittle = character_cleaner_by_name(doolittle_raw, 'DOOLITTLE', 'xxxxxx', 'xxxxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same characters, just been joined afterwards due to computational convenience\n",
    "pickering = pickering + gentelman\n",
    "higgins = higgins + taker\n",
    "liza = liza + girl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(251, 869, 495, 178, 285)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pickering), len(higgins), len(liza), len(mrshiggins), len(doolittle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Androcles and the Lion - George Bernard Shaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_aatl = paragraphs_cleaner(\"androcles_and_the_lion_shaw.txt\", normal_braquets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_aatl = token_paragraphs_aatl[16:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "lavinia_raw = []\n",
    "captain_raw = []\n",
    "androcles_raw = []\n",
    "megaera_raw = []\n",
    "centurion_raw = []\n",
    "spintho_raw = []\n",
    "ferrovius_raw = []\n",
    "for par in token_paragraphs_aatl:\n",
    "    if 'LAVINIA' in par:\n",
    "        lavinia_raw.append(par)\n",
    "    if 'CAPTAIN' in par:\n",
    "        captain_raw.append(par)\n",
    "    if 'ANDROCLES' in par:\n",
    "        androcles_raw.append(par)\n",
    "    if  'MEGAERA' in par:\n",
    "        megaera_raw.append(par)\n",
    "    if  'CENTURION' in par:\n",
    "        centurion_raw.append(par)\n",
    "    if  'SPINTHO' in par:\n",
    "        spintho_raw.append(par)\n",
    "    if  'FERROVIUS' in par:\n",
    "        ferrovius_raw.append(par)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "lavinia = character_cleaner_by_name(lavinia_raw, 'LAVINIA', 'xxxx', 'xxxx')\n",
    "captain = character_cleaner_by_name(captain_raw, 'CAPTAIN', 'xxxx', 'xxxx')\n",
    "androcles = character_cleaner_by_name(androcles_raw, 'ANDROCLES', 'xxxx', 'xxxx')\n",
    "megaera = character_cleaner_by_name(megaera_raw, 'MEGAERA', 'xxxx', 'xxxx')\n",
    "centurion = character_cleaner_by_name(centurion_raw, 'CENTURION', 'xxxx', 'xxxx')\n",
    "spintho = character_cleaner_by_name(spintho_raw, 'SPINTHO', 'xxxx', 'xxxx')\n",
    "ferrovius = character_cleaner_by_name(ferrovius_raw, 'FERROVIUS', 'xxxx', 'xxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171, 155, 218, 68, 93, 57, 184)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lavinia), len(captain), len(androcles), len(megaera), len(centurion), len(spintho),len(ferrovius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caesar and Cleopatra - George Bernard Shaw    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_cac = paragraphs_cleaner(\"caesar_and_cleopatra_shaw.txt\", normal_braquets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_cac = token_paragraphs_cac[18:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "caesar_raw = []\n",
    "cleopatra_raw = []\n",
    "pothinus_raw = []\n",
    "rufio_raw = []\n",
    "ftatateeta_raw =[]\n",
    "apollodorus_raw = []\n",
    "for par in token_paragraphs_cac:\n",
    "    if 'CAESAR' in par:\n",
    "        caesar_raw.append(par)\n",
    "    if 'CLEOPATRA' in par:\n",
    "        cleopatra_raw.append(par)\n",
    "    if 'POTHINUS' in par:\n",
    "        pothinus_raw.append(par)\n",
    "    if  'RUFIO' in par:\n",
    "        rufio_raw.append(par)\n",
    "    if  'FTATATEETA' in par:\n",
    "        ftatateeta_raw.append(par)\n",
    "    if  'APOLLODORUS' in par:\n",
    "        apollodorus_raw.append(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "caesar = character_cleaner_by_name(caesar_raw, 'CAESAR', 'xxxx', 'xxxx')\n",
    "cleopatra = character_cleaner_by_name(cleopatra_raw, 'CLEOPATRA', 'xxxx', 'xxxx')\n",
    "pothinus = character_cleaner_by_name(pothinus_raw, 'POTHINUS', 'xxxx', 'xxxx')\n",
    "rufio = character_cleaner_by_name(rufio_raw, 'RUFIO', 'xxxx', 'xxxx')\n",
    "ftatateeta = character_cleaner_by_name(ftatateeta_raw, 'FTATATEETA', 'xxxx', 'xxxx')\n",
    "apollodorus = character_cleaner_by_name(apollodorus_raw, 'APOLLODORUS', 'xxxx', 'xxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(756, 554, 118, 268, 110, 188)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(caesar), len(cleopatra), len(pothinus), len(rufio), len(ftatateeta), len(apollodorus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Candida - George Bernard Shaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_can = paragraphs_cleaner(\"candida_shaw.txt\", normal_braquets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_can = token_paragraphs_can[17:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "candida_raw = []\n",
    "marchbanks_raw =[]\n",
    "morell_raw = []\n",
    "burgess_raw = []\n",
    "proserpine_raw = []\n",
    "for par in token_paragraphs_can:\n",
    "    if 'CANDIDA' in par:\n",
    "        candida_raw.append(par)\n",
    "    if 'MARCHBANKS' in par:\n",
    "        marchbanks_raw.append(par)\n",
    "    if 'MORELL' in par:\n",
    "        morell_raw.append(par)\n",
    "    if  'BURGESS' in par:\n",
    "        burgess_raw.append(par)\n",
    "    if  'PROSERPINE' in par:\n",
    "        proserpine_raw.append(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "candida = character_cleaner_by_name(candida_raw, 'CANDIDA', 'xxxx', 'xxxx')\n",
    "marchbanks = character_cleaner_by_name(marchbanks_raw, 'MARCHBANKS', 'xxxx', 'xxxx')\n",
    "morell = character_cleaner_by_name(morell_raw, 'MORELL', 'xxxx', 'xxxx')\n",
    "burgess = character_cleaner_by_name(burgess_raw, 'BURGESS', 'xxxx', 'xxxx')\n",
    "proserpine = character_cleaner_by_name(proserpine_raw, 'PROSERPINE', 'xxxx', 'xxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(354, 388, 443, 238, 147)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candida), len(marchbanks), len(morell), len(burgess), len(proserpine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Man And Superman - George Bernard Shaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_mas = paragraphs_cleaner(\"man_and_superman_shaw.txt\", squared_braquets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_mas = token_paragraphs_mas[54:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramsden_raw = []\n",
    "octavius_raw = []\n",
    "tanner_raw = []\n",
    "ann_raw = []\n",
    "whitefield_raw = []\n",
    "missramsden_raw = []\n",
    "violet_raw = []\n",
    "hector_raw = []\n",
    "straker_raw = []\n",
    "mendoza_raw = []\n",
    "juan_raw = []\n",
    "devil_raw = []\n",
    "ana_raw = []\n",
    "for par in token_paragraphs_mas:\n",
    "    if  'RAMSDEN' in par:\n",
    "         ramsden_raw.append(par)\n",
    "    if  'OCTAVIUS' in par:\n",
    "         octavius_raw.append(par)\n",
    "    if  'TANNER' in par:\n",
    "         tanner_raw.append(par)\n",
    "    if  'ANN' in par:\n",
    "         ann_raw.append(par)\n",
    "    if  'WHITEFIELD' in par and 'MRS' or 'MRS.' in par:\n",
    "         whitefield_raw.append(par)\n",
    "    if  'RAMSDEN' in par and 'MISS' in par:\n",
    "         missramsden_raw.append(par)\n",
    "    if  'VIOLET' in par:\n",
    "         violet_raw.append(par)\n",
    "    if  'HECTOR' in par:\n",
    "         hector_raw.append(par)\n",
    "    if  'STRAKER' in par:\n",
    "         straker_raw.append(par)\n",
    "    if  'MENDOZA' in par:\n",
    "         mendoza_raw.append(par)\n",
    "    if  'JUAN' in par:\n",
    "         juan_raw.append(par)\n",
    "    if  'DEVIL' in par:\n",
    "         devil_raw.append(par)\n",
    "    if  'ANA' in par:\n",
    "         ana_raw.append(par)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramsden = character_cleaner_by_name(ramsden_raw, 'RAMSDEN', 'xxxx', 'xxxx')\n",
    "octavius = character_cleaner_by_name(octavius_raw, 'OCTAVIUS', 'xxxx', 'xxxx')\n",
    "tanner = character_cleaner_by_name(tanner_raw, 'TANNER', 'xxxx', 'xxxx')\n",
    "ann = character_cleaner_by_name(ann_raw, 'ANN', 'xxxx', 'xxxx')\n",
    "missramsden = character_cleaner_by_name(whitefield_raw, 'WHITEFIELD', 'MRS', 'MRS.')\n",
    "violet = character_cleaner_by_name(violet_raw, 'VIOLET', 'xxxx', 'xxxx')\n",
    "hector = character_cleaner_by_name(hector_raw, 'HECTOR', 'xxxx', 'xxxx')\n",
    "straker = character_cleaner_by_name(straker_raw, 'STRAKER', 'xxxx', 'xxxx')\n",
    "mendoza = character_cleaner_by_name(mendoza_raw, 'MENDOZA', 'xxxx', 'xxxx')\n",
    "juan = character_cleaner_by_name(juan_raw, 'JUAN', 'DON', 'xxxx')\n",
    "devil = character_cleaner_by_name(devil_raw, 'DEVIL', 'THE', 'xxxx')\n",
    "ana = character_cleaner_by_name(ana_raw, 'ANA', 'xxxx', 'xxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(293, 267, 970, 419, 97, 162)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ramsden), len(octavius), len(tanner), len(ann), len(missramsden), len(violet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 159, 192, 516, 237, 131)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hector), len(straker), len(mendoza), len(juan), len(devil), len(ana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
