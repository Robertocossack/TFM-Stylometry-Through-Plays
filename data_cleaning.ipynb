{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_braquets = \"\\[.*?]\"\n",
    "normal_braquets = \"\\(.*?\\)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraphs_cleaner(file,regex): #Two in one function\n",
    "    f = open(file, \"r\")\n",
    "    contents = f.read()         \n",
    "    f.close()     \n",
    "    paragraph_cutter = re.sub('\\n{2,}', '\\n\\n', contents) #Paragraph identifier\n",
    "    paragraphs = paragraph_cutter.split('\\n\\n') #It cuts the paragraphs by the identifier points\n",
    "    clean_result = []\n",
    "    for par in paragraphs:\n",
    "        output = re.sub(\"\\\\n\", \" \", par) # Removes the new-line characters within the paragraphs\n",
    "        clean = re.sub(regex , \"\", output) # Removes the additional text in brackets\n",
    "        clean_result.append(clean)\n",
    "    result = []\n",
    "    for par in clean_result:\n",
    "        output = nltk.word_tokenize(par) #Paragraph tokenizer\n",
    "        result.append(output)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_paragraphs(file,regex):\n",
    "    f = open(file, \"r\")\n",
    "    contents = f.read()         \n",
    "    f.close()     \n",
    "    paragraph_cutter = re.sub('\\n{2,}', '\\n\\n', contents) #Identifica los párrafos\n",
    "    paragraphs = paragraph_cutter.split('\\n\\n') #Corta los parrafos por puntos de identificación\n",
    "    result = []\n",
    "    for par in paragraphs:\n",
    "        output = re.sub(\"\\\\n\", \" \", par) # Elimina los caracteres de salto de linea dentro de los parrafos\n",
    "        clean = re.sub(regex , \"\", output) #Elimina los brackets de comentarios situacionales\n",
    "        result.append(clean)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_paragraph_tokenizer(clean_result):    \n",
    "    result = []\n",
    "    for par in clean_result:\n",
    "        output = nltk.word_tokenize(par) #Toqueniza los párrafos\n",
    "        result.append(output)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_cutter (character_raw, n): #Where n is the number of the cut position\n",
    "    result = []\n",
    "    for sen in character_raw:\n",
    "        output = sen[n:]\n",
    "        result.append(output)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_joiner(character):\n",
    "    output = []\n",
    "    for par in character:\n",
    "        for word in par:\n",
    "            output.append(word)\n",
    "    one_string = \" \".join(output)\n",
    "    result = nltk.sent_tokenize(one_string)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_cleaner (character_raw, n): #Two in one function\n",
    "    no_name = []\n",
    "    for sen in character_raw:\n",
    "        output = sen[n:]\n",
    "        no_name.append(output)\n",
    "    output = []\n",
    "    for par in no_name:\n",
    "        for word in par:\n",
    "            output.append(word)\n",
    "    one_string = \" \".join(output)\n",
    "    result = nltk.sent_tokenize(one_string)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An Ideal Husband - Oscar Wilde\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_aih = paragraphs_cleaner(\"an_ideal_husband_wilde.txt\", squared_braquets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_aih = token_paragraphs_aih[48:]  #Comienzo de la obra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "caversham_raw = []\n",
    "goring_raw = []\n",
    "chiltern_raw = []\n",
    "ladychiltern_raw = []\n",
    "mabel_raw = []\n",
    "cheveley_raw = []\n",
    "for par in token_paragraphs_aih:\n",
    "    if 'CAVERSHAM' in par:\n",
    "        caversham_raw.append(par)\n",
    "    if 'GORING' in par:\n",
    "        goring_raw.append(par)\n",
    "    if 'ROBERT' in par:\n",
    "        chiltern_raw.append(par)\n",
    "    if  'LADY' and 'CHILTERN' in par and not 'MABEL' in par and not 'SIR' in par:\n",
    "        ladychiltern_raw.append(par)\n",
    "    if 'MABEL' in par:\n",
    "        mabel_raw.append(par)\n",
    "    if 'CHEVELEY' in par:\n",
    "        cheveley_raw.append(par)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "caversham = character_cleaner(caversham_raw, 3)\n",
    "goring = character_cleaner(goring_raw, 3)\n",
    "chiltern = character_cleaner(chiltern_raw, 4)\n",
    "ladychiltern = character_cleaner(ladychiltern_raw, 3)\n",
    "mabel = character_cleaner(mabel_raw, 3)\n",
    "cheveley = character_cleaner(cheveley_raw, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213, 796, 537, 381, 242, 534)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(caversham), len(goring), len(chiltern), len(ladychiltern), len(mabel), len(cheveley)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Woman Of No Importance - Oscar Wilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_awoni = paragraphs_cleaner(\"a_woman_of_no_importance_wilde.txt\", squared_braquets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    " token_paragraphs_awoni = token_paragraphs_awoni[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "illingworth_raw = []\n",
    "allonby_raw = []\n",
    "gerald_raw = []\n",
    "mrsarbuthnot_raw = []\n",
    "hunstanton_raw = []\n",
    "hester_raw = []\n",
    "\n",
    "for par in token_paragraphs_awoni:\n",
    "    if 'ILLINGWORTH' in par:\n",
    "        illingworth_raw.append(par)\n",
    "    if 'ALLONBY' in par:\n",
    "        allonby_raw.append(par)\n",
    "    if 'GERALD' in par:\n",
    "        gerald_raw.append(par)\n",
    "    if  'MRS' and 'ARBUTHNOT' in par:\n",
    "        mrsarbuthnot_raw.append(par)\n",
    "    if 'HUNSTANTON' in par:\n",
    "        hunstanton_raw.append(par)\n",
    "    if 'HESTER' in par:\n",
    "        hester_raw.append(par)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "illingworth = character_cleaner(illingworth_raw, 3)\n",
    "allonby = character_cleaner(allonby_raw, 3)\n",
    "gerald = character_cleaner(gerald_raw, 2)\n",
    "mrsarbuthnot = character_cleaner(mrsarbuthnot_raw, 2)\n",
    "hunstanton = character_cleaner(hunstanton_raw, 2)\n",
    "hester = character_cleaner(hester_raw, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(428, 213, 259, 498, 479, 142)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(illingworth), len(allonby), len(gerald), len(mrsarbuthnot), len(hunstanton), len(hester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
