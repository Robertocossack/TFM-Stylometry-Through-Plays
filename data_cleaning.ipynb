{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_braquets = \"\\[.*?]\"\n",
    "normal_braquets = \"\\(.*?\\)\"\n",
    "squared_to_point = \"\\\\[.*?\\.\"\n",
    "normal_to_point = \"\\\\(.*?\\.\"\n",
    "nothing = \"xxxxxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraphs_cleaner(file,regex, regex1): #Two in one function\n",
    "    f = open(file, \"r\")\n",
    "    contents = f.read()         \n",
    "    f.close()     \n",
    "    paragraph_cutter = re.sub('\\n{2,}', '\\n\\n', contents) #Paragraph identifier\n",
    "    paragraphs = paragraph_cutter.split('\\n\\n') #It cuts the paragraphs by the identifier points\n",
    "    clean_result = []\n",
    "    for par in paragraphs:\n",
    "        output = re.sub(\"\\\\n\", \" \", par) # Removes the new-line characters within the paragraphs\n",
    "        pseudo_clean = re.sub(regex , \"\", output) # Removes the additional text in brackets\n",
    "        clean = re.sub(regex1 , \"\", pseudo_clean)\n",
    "        clean_result.append(clean)\n",
    "    result = []\n",
    "    for par in clean_result:\n",
    "        output = nltk.word_tokenize(par) #Paragraph tokenizer\n",
    "        result.append(output)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_paragraph_tokenizer(clean_result):    \n",
    "    result = []\n",
    "    for par in clean_result:\n",
    "        output = nltk.word_tokenize(par) #Tokenize words inside the paragraphs\n",
    "        result.append(output)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_name_cutter (character_raw, n): #Where n is the number of the cut position\n",
    "    result = []\n",
    "    for sen in character_raw:\n",
    "        output = sen[n:]\n",
    "        result.append(output)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_joiner(character): # Join the strings and tokenize the result by sentence\n",
    "    output = []\n",
    "    for par in character:\n",
    "        for word in par:\n",
    "            output.append(word)\n",
    "    one_string = \" \".join(output)\n",
    "    result = nltk.sent_tokenize(one_string)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_cleaner (character_raw, n): #Two in one function\n",
    "    no_name = []\n",
    "    for sen in character_raw:\n",
    "        output = sen[n:]\n",
    "        no_name.append(output)\n",
    "    output = []\n",
    "    for par in no_name:\n",
    "        for word in par:\n",
    "            output.append(word)\n",
    "    one_string = \" \".join(output)\n",
    "    result = nltk.sent_tokenize(one_string)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_cleaner_by_name (character_raw, name1, name2, name3):\n",
    "    for sen in character_raw:\n",
    "        if name1 in sen:\n",
    "            sen.remove(name1)\n",
    "    for sen in character_raw:\n",
    "        if name2 in sen:\n",
    "            sen.remove(name2)\n",
    "    for sen in character_raw:\n",
    "        if name3 in sen:\n",
    "            sen.remove(name3)\n",
    "    output = string_joiner(character_raw)\n",
    "    for sen in  output: #Removes the single points sentences for a best estimation of the lenghth\n",
    "        if sen == '.':\n",
    "            output.remove(sen)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_cutter (character_raw, name1, name2, name3):\n",
    "    for sen in character_raw:\n",
    "        if name1 in sen:\n",
    "            sen.remove(name1)\n",
    "    for sen in character_raw:\n",
    "        if name2 in sen:\n",
    "            sen.remove(name2)\n",
    "    for sen in character_raw:\n",
    "        if name3 in sen:\n",
    "            sen.remove(name3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An Ideal Husband - Oscar Wilde\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_aih = paragraphs_cleaner(\"an_ideal_husband_wilde.txt\", squared_braquets, nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_aih = token_paragraphs_aih[48:]  #Comienzo de la obra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "caversham_raw = []\n",
    "goring_raw = []\n",
    "chiltern_raw = []\n",
    "ladychiltern_raw = []\n",
    "mabel_raw = []\n",
    "cheveley_raw = []\n",
    "for par in token_paragraphs_aih:\n",
    "    if 'CAVERSHAM' in par:\n",
    "        caversham_raw.append(par)\n",
    "    if 'GORING' in par:\n",
    "        goring_raw.append(par)\n",
    "    if 'ROBERT' in par:\n",
    "        chiltern_raw.append(par)\n",
    "    if  'LADY' and 'CHILTERN' in par and not 'MABEL' in par and not 'SIR' in par:\n",
    "        ladychiltern_raw.append(par)\n",
    "    if 'MABEL' in par:\n",
    "        mabel_raw.append(par)\n",
    "    if 'CHEVELEY' in par:\n",
    "        cheveley_raw.append(par)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "caversham = character_cleaner(caversham_raw, 3)\n",
    "goring = character_cleaner(goring_raw, 3)\n",
    "chiltern = character_cleaner(chiltern_raw, 4)\n",
    "ladychiltern = character_cleaner(ladychiltern_raw, 3)\n",
    "mabel = character_cleaner(mabel_raw, 3)\n",
    "cheveley = character_cleaner(cheveley_raw, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213, 796, 537, 381, 242, 534)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(caversham), len(goring), len(chiltern), len(ladychiltern), len(mabel), len(cheveley)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Woman of No Importance - Oscar Wilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_awoni = paragraphs_cleaner(\"a_woman_of_no_importance_wilde.txt\", squared_braquets, nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    " token_paragraphs_awoni = token_paragraphs_awoni[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "illingworth_raw = []\n",
    "allonby_raw = []\n",
    "gerald_raw = []\n",
    "mrsarbuthnot_raw = []\n",
    "hunstanton_raw = []\n",
    "hester_raw = []\n",
    "\n",
    "for par in token_paragraphs_awoni:\n",
    "    if 'ILLINGWORTH' in par:\n",
    "        illingworth_raw.append(par)\n",
    "    if 'ALLONBY' in par:\n",
    "        allonby_raw.append(par)\n",
    "    if 'GERALD' in par:\n",
    "        gerald_raw.append(par)\n",
    "    if  'MRS' and 'ARBUTHNOT' in par:\n",
    "        mrsarbuthnot_raw.append(par)\n",
    "    if 'HUNSTANTON' in par:\n",
    "        hunstanton_raw.append(par)\n",
    "    if 'HESTER' in par:\n",
    "        hester_raw.append(par)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "illingworth = character_cleaner(illingworth_raw, 3)\n",
    "allonby = character_cleaner(allonby_raw, 3)\n",
    "gerald = character_cleaner(gerald_raw, 2)\n",
    "mrsarbuthnot = character_cleaner(mrsarbuthnot_raw, 2)\n",
    "hunstanton = character_cleaner(hunstanton_raw, 2)\n",
    "hester = character_cleaner(hester_raw, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(428, 213, 259, 498, 479, 142)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(illingworth), len(allonby), len(gerald), len(mrsarbuthnot), len(hunstanton), len(hester)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Lady Windermer's Fan - Oscar Wilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_lwf = paragraphs_cleaner(\"lady_windermeres_fan_wilde.txt\", squared_braquets, nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_lwf = token_paragraphs_lwf[47:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "lordwindermere_raw = []\n",
    "erlynne_raw = []\n",
    "augustus_raw = []\n",
    "windermere_raw = []\n",
    "darlington_raw = []\n",
    "berwick_raw = []\n",
    "for par in token_paragraphs_lwf:\n",
    "    if 'LORD' and 'WINDERMERE' in par and not 'LADY' in par:\n",
    "        lordwindermere_raw.append(par)\n",
    "    if 'ERLYNNE' in par:\n",
    "        erlynne_raw.append(par)\n",
    "    if 'AUGUSTUS' in par:\n",
    "        augustus_raw.append(par)\n",
    "    if  'LADY' and 'WINDERMERE' in par:\n",
    "        windermere_raw.append(par)\n",
    "    if  'DARLINGTON' in par:\n",
    "        darlington_raw.append(par)\n",
    "    if  'BERWICK' in par:\n",
    "        berwick_raw.append(par)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "lordwindermere = character_cleaner(lordwindermere_raw, 3)\n",
    "erlynne = character_cleaner(erlynne_raw, 3)\n",
    "augustus = character_cleaner(augustus_raw, 3)\n",
    "windermere = character_cleaner(windermere_raw, 3)\n",
    "darlington = character_cleaner(darlington_raw,3)\n",
    "berwick = character_cleaner(berwick_raw, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234, 395, 95, 743, 192, 234)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lordwindermere), len(erlynne), len(augustus), len(windermere), len(darlington), len(berwick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Importance of Being Earnest - Oscar Wilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_tiobe = paragraphs_cleaner(\"the_importance_of_being_earnest_wilde.txt\", squared_braquets, nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_tiobe = token_paragraphs_tiobe[25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "jack_raw = []\n",
    "algernon_raw = []\n",
    "gwendolen_raw = []\n",
    "bracknell_raw = []\n",
    "cecily_raw = []\n",
    "prism_raw = []\n",
    "chasuble_raw = []\n",
    "for par in token_paragraphs_tiobe:\n",
    "    if 'Jack' in par[0:1]:\n",
    "        jack_raw.append(par)\n",
    "    if 'Algernon' in par[0:1]:\n",
    "        algernon_raw.append(par)\n",
    "    if 'Gwendolen' in par[0:1]:\n",
    "        gwendolen_raw.append(par)\n",
    "    if  'Bracknell' in par[0:2]:\n",
    "        bracknell_raw.append(par)\n",
    "    if  'Cecily' in par[0:1]:\n",
    "        cecily_raw.append(par)\n",
    "    if  'Prism' in par[0:2]:\n",
    "        prism_raw.append(par)\n",
    "    if  'Chasuble' in par[0:1]:\n",
    "        chasuble_raw.append(par)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "jack = character_cleaner(jack_raw, 2)\n",
    "algernon = character_cleaner(algernon_raw, 2)\n",
    "gwendolen = character_cleaner(gwendolen_raw, 2)\n",
    "bracknell = character_cleaner(bracknell_raw, 3)\n",
    "cecily = character_cleaner(cecily_raw, 3)\n",
    "prism = character_cleaner(prism_raw, 3)\n",
    "chasuble = character_cleaner(chasuble_raw, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(492, 451, 247, 285, 296, 102, 82)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jack), len(algernon), len(gwendolen), len(bracknell), len(cecily), len(prism), len(chasuble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pygmalion - George Bernard Shaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_p = paragraphs_cleaner(\"pygmalion_shaw.txt\", squared_braquets, nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_p = token_paragraphs_p[21:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickering_raw = []\n",
    "higgins_raw = []\n",
    "liza_raw = []\n",
    "gentleman_raw = []\n",
    "taker_raw = []\n",
    "girl_raw = []\n",
    "mrshiggins_raw = []\n",
    "doolittle_raw = []\n",
    "for par in token_paragraphs_p:\n",
    "    if 'PICKERING' in par:\n",
    "        pickering_raw.append(par)\n",
    "    if 'HIGGINS' in par and not 'MRS.'in par:\n",
    "        higgins_raw.append(par)\n",
    "    if 'LIZA' in par:\n",
    "        liza_raw.append(par)\n",
    "    if  'GENTLEMAN' in par:\n",
    "        gentleman_raw.append(par)\n",
    "    if  'TAKER' in par:\n",
    "        taker_raw.append(par)\n",
    "    if  'GIRL' in par:\n",
    "        girl_raw.append(par)\n",
    "    if  'MRS.' in par and 'HIGGINS' in par[0:2]:\n",
    "        mrshiggins_raw.append(par)\n",
    "    if  'DOOLITTLE' in par:\n",
    "        doolittle_raw.append(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickering = character_cleaner_by_name(pickering_raw, 'PICKERING', 'xxxxxx', 'xxxxxx')\n",
    "higgins = character_cleaner_by_name(higgins_raw, 'HIGGINS', 'xxxxx', 'xxxxx')\n",
    "liza = character_cleaner_by_name(liza_raw, 'LIZA', 'xxxxx', 'xxxxx')\n",
    "gentelman = character_cleaner_by_name(gentleman_raw, 'THE', 'GENTLEMAN', 'xxxxx')\n",
    "taker = character_cleaner_by_name(taker_raw, 'THE', 'NOTE', 'TAKER')\n",
    "girl = character_cleaner_by_name(girl_raw, 'THE', 'FLOWER', 'GIRL')\n",
    "mrshiggins = character_cleaner_by_name(mrshiggins_raw, 'MRS', 'MRS.', 'HIGGINS')\n",
    "doolittle = character_cleaner_by_name(doolittle_raw, 'DOOLITTLE', 'xxxxxx', 'xxxxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same characters, just been joined afterwards due to computational convenience\n",
    "pickering = pickering + gentelman\n",
    "higgins = higgins + taker\n",
    "liza = liza + girl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(251, 869, 495, 178, 285)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pickering), len(higgins), len(liza), len(mrshiggins), len(doolittle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Androcles and the Lion - George Bernard Shaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_aatl = paragraphs_cleaner(\"androcles_and_the_lion_shaw.txt\", normal_braquets, nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_aatl = token_paragraphs_aatl[16:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "lavinia_raw = []\n",
    "captain_raw = []\n",
    "androcles_raw = []\n",
    "megaera_raw = []\n",
    "centurion_raw = []\n",
    "spintho_raw = []\n",
    "ferrovius_raw = []\n",
    "for par in token_paragraphs_aatl:\n",
    "    if 'LAVINIA' in par:\n",
    "        lavinia_raw.append(par)\n",
    "    if 'CAPTAIN' in par:\n",
    "        captain_raw.append(par)\n",
    "    if 'ANDROCLES' in par:\n",
    "        androcles_raw.append(par)\n",
    "    if  'MEGAERA' in par:\n",
    "        megaera_raw.append(par)\n",
    "    if  'CENTURION' in par:\n",
    "        centurion_raw.append(par)\n",
    "    if  'SPINTHO' in par:\n",
    "        spintho_raw.append(par)\n",
    "    if  'FERROVIUS' in par:\n",
    "        ferrovius_raw.append(par)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "lavinia = character_cleaner_by_name(lavinia_raw, 'LAVINIA', 'xxxx', 'xxxx')\n",
    "captain = character_cleaner_by_name(captain_raw, 'CAPTAIN', 'xxxx', 'xxxx')\n",
    "androcles = character_cleaner_by_name(androcles_raw, 'ANDROCLES', 'xxxx', 'xxxx')\n",
    "megaera = character_cleaner_by_name(megaera_raw, 'MEGAERA', 'xxxx', 'xxxx')\n",
    "centurion = character_cleaner_by_name(centurion_raw, 'CENTURION', 'xxxx', 'xxxx')\n",
    "spintho = character_cleaner_by_name(spintho_raw, 'SPINTHO', 'xxxx', 'xxxx')\n",
    "ferrovius = character_cleaner_by_name(ferrovius_raw, 'FERROVIUS', 'xxxx', 'xxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173, 155, 219, 68, 93, 57, 184)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lavinia), len(captain), len(androcles), len(megaera), len(centurion), len(spintho),len(ferrovius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caesar and Cleopatra - George Bernard Shaw    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_cac = paragraphs_cleaner(\"caesar_and_cleopatra_shaw.txt\", normal_braquets, nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_cac = token_paragraphs_cac[18:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "caesar_raw = []\n",
    "cleopatra_raw = []\n",
    "pothinus_raw = []\n",
    "rufio_raw = []\n",
    "ftatateeta_raw =[]\n",
    "apollodorus_raw = []\n",
    "for par in token_paragraphs_cac:\n",
    "    if 'CAESAR' in par:\n",
    "        caesar_raw.append(par)\n",
    "    if 'CLEOPATRA' in par:\n",
    "        cleopatra_raw.append(par)\n",
    "    if 'POTHINUS' in par:\n",
    "        pothinus_raw.append(par)\n",
    "    if  'RUFIO' in par:\n",
    "        rufio_raw.append(par)\n",
    "    if  'FTATATEETA' in par:\n",
    "        ftatateeta_raw.append(par)\n",
    "    if  'APOLLODORUS' in par:\n",
    "        apollodorus_raw.append(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "caesar = character_cleaner_by_name(caesar_raw, 'CAESAR', 'xxxx', 'xxxx')\n",
    "cleopatra = character_cleaner_by_name(cleopatra_raw, 'CLEOPATRA', 'xxxx', 'xxxx')\n",
    "pothinus = character_cleaner_by_name(pothinus_raw, 'POTHINUS', 'xxxx', 'xxxx')\n",
    "rufio = character_cleaner_by_name(rufio_raw, 'RUFIO', 'xxxx', 'xxxx')\n",
    "ftatateeta = character_cleaner_by_name(ftatateeta_raw, 'FTATATEETA', 'xxxx', 'xxxx')\n",
    "apollodorus = character_cleaner_by_name(apollodorus_raw, 'APOLLODORUS', 'xxxx', 'xxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(756, 554, 118, 268, 110, 188)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(caesar), len(cleopatra), len(pothinus), len(rufio), len(ftatateeta), len(apollodorus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Candida - George Bernard Shaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_can = paragraphs_cleaner(\"candida_shaw.txt\", normal_braquets, nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_can = token_paragraphs_can[17:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "candida_raw = []\n",
    "marchbanks_raw =[]\n",
    "morell_raw = []\n",
    "burgess_raw = []\n",
    "proserpine_raw = []\n",
    "for par in token_paragraphs_can:\n",
    "    if 'CANDIDA' in par:\n",
    "        candida_raw.append(par)\n",
    "    if 'MARCHBANKS' in par:\n",
    "        marchbanks_raw.append(par)\n",
    "    if 'MORELL' in par:\n",
    "        morell_raw.append(par)\n",
    "    if  'BURGESS' in par:\n",
    "        burgess_raw.append(par)\n",
    "    if  'PROSERPINE' in par:\n",
    "        proserpine_raw.append(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "candida = character_cleaner_by_name(candida_raw, 'CANDIDA', 'xxxx', 'xxxx')\n",
    "marchbanks = character_cleaner_by_name(marchbanks_raw, 'MARCHBANKS', 'xxxx', 'xxxx')\n",
    "morell = character_cleaner_by_name(morell_raw, 'MORELL', 'xxxx', 'xxxx')\n",
    "burgess = character_cleaner_by_name(burgess_raw, 'BURGESS', 'xxxx', 'xxxx')\n",
    "proserpine = character_cleaner_by_name(proserpine_raw, 'PROSERPINE', 'xxxx', 'xxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(354, 388, 443, 238, 147)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candida), len(marchbanks), len(morell), len(burgess), len(proserpine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Man And Superman - George Bernard Shaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_mas = paragraphs_cleaner(\"man_and_superman_shaw.txt\", squared_braquets, nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_mas = token_paragraphs_mas[54:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramsden_raw = []\n",
    "octavius_raw = []\n",
    "tanner_raw = []\n",
    "ann_raw = []\n",
    "whitefield_raw = []\n",
    "missramsden_raw = []\n",
    "violet_raw = []\n",
    "hector_raw = []\n",
    "straker_raw = []\n",
    "mendoza_raw = []\n",
    "juan_raw = []\n",
    "devil_raw = []\n",
    "ana_raw = []\n",
    "for par in token_paragraphs_mas:\n",
    "    if  'RAMSDEN' in par:\n",
    "         ramsden_raw.append(par)\n",
    "    if  'OCTAVIUS' in par:\n",
    "         octavius_raw.append(par)\n",
    "    if  'TANNER' in par:\n",
    "         tanner_raw.append(par)\n",
    "    if  'ANN' in par:\n",
    "         ann_raw.append(par)\n",
    "    if  'WHITEFIELD' in par and 'MRS' or 'MRS.' in par:\n",
    "         whitefield_raw.append(par)\n",
    "    if  'RAMSDEN' in par and 'MISS' in par:\n",
    "         missramsden_raw.append(par)\n",
    "    if  'VIOLET' in par:\n",
    "         violet_raw.append(par)\n",
    "    if  'HECTOR' in par:\n",
    "         hector_raw.append(par)\n",
    "    if  'STRAKER' in par:\n",
    "         straker_raw.append(par)\n",
    "    if  'MENDOZA' in par:\n",
    "         mendoza_raw.append(par)\n",
    "    if  'JUAN' in par:\n",
    "         juan_raw.append(par)\n",
    "    if  'DEVIL' in par:\n",
    "         devil_raw.append(par)\n",
    "    if  'ANA' in par:\n",
    "         ana_raw.append(par)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramsden = character_cleaner_by_name(ramsden_raw, 'RAMSDEN', 'xxxx', 'xxxx')\n",
    "octavius = character_cleaner_by_name(octavius_raw, 'OCTAVIUS', 'xxxx', 'xxxx')\n",
    "tanner = character_cleaner_by_name(tanner_raw, 'TANNER', 'xxxx', 'xxxx')\n",
    "ann = character_cleaner_by_name(ann_raw, 'ANN', 'xxxx', 'xxxx')\n",
    "missramsden = character_cleaner_by_name(whitefield_raw, 'WHITEFIELD', 'MRS', 'MRS.')\n",
    "violet = character_cleaner_by_name(violet_raw, 'VIOLET', 'xxxx', 'xxxx')\n",
    "hector = character_cleaner_by_name(hector_raw, 'HECTOR', 'xxxx', 'xxxx')\n",
    "straker = character_cleaner_by_name(straker_raw, 'STRAKER', 'xxxx', 'xxxx')\n",
    "mendoza = character_cleaner_by_name(mendoza_raw, 'MENDOZA', 'xxxx', 'xxxx')\n",
    "juan = character_cleaner_by_name(juan_raw, 'JUAN', 'DON', 'xxxx')\n",
    "devil = character_cleaner_by_name(devil_raw, 'DEVIL', 'THE', 'xxxx')\n",
    "ana = character_cleaner_by_name(ana_raw, 'ANA', 'xxxx', 'xxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(293, 267, 970, 419, 97, 162)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ramsden), len(octavius), len(tanner), len(ann), len(missramsden), len(violet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 159, 192, 516, 237, 131)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hector), len(straker), len(mendoza), len(juan), len(devil), len(ana)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cyntia's Revels - Ben Jonson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_cr = paragraphs_cleaner(\"cynthias_revels_jonson.txt\", squared_braquets, nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_cr = token_paragraphs_cr[153:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "mercury_raw = []\n",
    "echo_raw = []\n",
    "cupid_raw = []\n",
    "amorphus_raw = []\n",
    "asotus_raw = []\n",
    "crites_raw = []\n",
    "anaides_raw = []\n",
    "hedon_raw = []\n",
    "arete_raw = []\n",
    "phantase_raw = []\n",
    "philautia_raw = []\n",
    "cyntia_raw = []\n",
    "for par in token_paragraphs_cr:\n",
    "    if  'MER' in par:\n",
    "         mercury_raw.append(par)\n",
    "    if  'ECHO' in par:\n",
    "         echo_raw.append(par)\n",
    "    if  'CUP' in par:\n",
    "         cupid_raw.append(par)\n",
    "    if  'AMO' in par:\n",
    "         amorphus_raw.append(par)\n",
    "    if  'ASO' in par:\n",
    "         asotus_raw.append(par)\n",
    "    if  'CRI' in par:\n",
    "         crites_raw.append(par)\n",
    "    if  'ANA' in par:\n",
    "         anaides_raw.append(par)\n",
    "    if  'HED' in par:\n",
    "         hedon_raw.append(par)\n",
    "    if  'ARE' in par:\n",
    "         arete_raw.append(par)\n",
    "    if  'PHA' in par:\n",
    "         phantase_raw.append(par)\n",
    "    if  'PHI' in par:\n",
    "         philautia_raw.append(par)\n",
    "    if  'CYN' in par:\n",
    "         cyntia_raw.append(par)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "mercury = character_cleaner_by_name(mercury_raw, 'MER', 'xxxx', 'xxxx')\n",
    "echo = character_cleaner_by_name(echo_raw, 'ECHO', 'xxxx', 'xxxx')\n",
    "cupid = character_cleaner_by_name(cupid_raw, 'CUP', 'xxxx', 'xxxx')\n",
    "amorphus = character_cleaner_by_name(amorphus_raw, 'AMO', 'xxxx', 'xxxx')\n",
    "asotus = character_cleaner_by_name(asotus_raw, 'ASO', 'xxxx', 'xxxx')\n",
    "crites = character_cleaner_by_name(crites_raw, 'CRI', 'xxxx', 'xxxx')\n",
    "anaides = character_cleaner_by_name(anaides_raw, 'ANA', 'xxxx', 'xxxx')\n",
    "hedon = character_cleaner_by_name(hedon_raw, 'HED', 'xxxx', 'xxxx')\n",
    "arete = character_cleaner_by_name(arete_raw, 'ARE', 'xxxx', 'xxxx')\n",
    "phantase = character_cleaner_by_name(phantase_raw, 'PHA', 'xxxx', 'xxxx')\n",
    "philautia = character_cleaner_by_name(philautia_raw, 'PHI', 'xxxx', 'xxxx')\n",
    "cyntia = character_cleaner_by_name(cyntia_raw, 'CYN', 'xxxx', 'xxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(293, 22, 148, 389, 216, 175)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mercury), len(echo), len(cupid), len(amorphus), len(asotus), len(crites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164, 147, 37, 149, 91, 46)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anaides), len(hedon), len(arete), len(phantase), len(philautia), len(cyntia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Every Man On His Humor - Ben Jonson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_emohh = paragraphs_cleaner(\"every_man_on_his_humour_jonson.txt\", squared_braquets, squared_to_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_emohh = token_paragraphs_emohh[82:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "edknowell_raw = []\n",
    "stephen_raw = []\n",
    "mathew_raw = []\n",
    "cob_raw = []\n",
    "bobadill_raw = []\n",
    "tib_raw = []\n",
    "kitely_raw = []\n",
    "cash_raw = []\n",
    "downright_raw = []\n",
    "damekitely_raw = []\n",
    "brainworm_raw = []\n",
    "knowell_raw = []\n",
    "wellbred_raw = []\n",
    "for par in token_paragraphs_emohh:\n",
    "    if  'Know' in par and 'E.' in par[0:1]:\n",
    "         edknowell_raw.append(par)\n",
    "    if  'Step' in par[0:2]:\n",
    "         stephen_raw.append(par)\n",
    "    if  'Mat' in par[0:2]:\n",
    "         mathew_raw.append(par)\n",
    "    if  'Cob' in par[0:2]:\n",
    "         cob_raw.append(par)\n",
    "    if  'Bob' in par[0:2]:\n",
    "         bobadill_raw.append(par)\n",
    "    if  'Tib' in par[0:2]:\n",
    "         tib_raw.append(par)\n",
    "    if  'Kit' in par[0:2]:\n",
    "         kitely_raw.append(par)\n",
    "    if  'Cash' in par[0:2]:\n",
    "         cash_raw.append(par)\n",
    "    if  'Dow' in par[0:2]:\n",
    "         downright_raw.append(par)\n",
    "    if  'Dame' in par[0:2] and 'K.' in par[0:3]:\n",
    "         damekitely_raw.append(par)\n",
    "    if  'Brai' in par[0:2]:\n",
    "         brainworm_raw.append(par)\n",
    "    if  'Know' in par[0:2] and not 'E.' in par:\n",
    "         knowell_raw.append(par)\n",
    "    if  'Wel' in par[0:2]:\n",
    "         wellbred_raw.append(par)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "stephen = character_cleaner_by_name(stephen_raw, 'Step', 'xxxx', 'xxxx')\n",
    "edknowell = character_cleaner_by_name(edknowell_raw, 'E.', 'Know', 'xxxx')\n",
    "mathew = character_cleaner_by_name(mathew_raw, 'Mat', 'xxxx', 'xxxx')\n",
    "cob = character_cleaner_by_name(cob_raw, 'Cob', 'xxxx', 'xxxx')\n",
    "bobadill = character_cleaner_by_name(bobadill_raw, 'Bob', 'xxxx', 'xxxx')\n",
    "tib = character_cleaner_by_name(tib_raw, 'Tib', 'xxxx', 'xxxx')\n",
    "kitely = character_cleaner_by_name(kitely_raw, 'Kit', 'xxxx', 'xxxx')\n",
    "cash = character_cleaner_by_name(cash_raw, 'Cash', 'xxxx', 'xxxx')\n",
    "downright = character_cleaner_by_name(downright_raw, 'Dow', 'xxxx', 'xxxx')\n",
    "damekitely = character_cleaner_by_name(damekitely_raw, 'Dame', 'K', 'xxxx')\n",
    "brainworm = character_cleaner_by_name(brainworm_raw, 'Brai', 'xxxx', 'xxxx')\n",
    "knowell = character_cleaner_by_name(knowell_raw, 'Know', 'xxxx', 'xxxx')\n",
    "wellbred = character_cleaner_by_name(wellbred_raw, 'Wel', 'xxxx', 'xxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170, 186, 142, 148, 195, 31, 268)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stephen), len(edknowell), len(mathew), len(cob), len(bobadill), len(tib), len(kitely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 107, 54, 171, 133, 145)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cash), len(downright), len(damekitely), len(brainworm), len(knowell), len(wellbred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Volpone, Or The Fox - Ben jonson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_votf = paragraphs_cleaner(\"volpone_or_the_fox_jonson.txt\", squared_braquets, nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_votf = token_paragraphs_votf[110:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "volpone_raw = []\n",
    "mosca_raw = []\n",
    "nano_raw = []\n",
    "androgyno_raw = []\n",
    "voltore_raw = []\n",
    "corbaccio_raw = []\n",
    "corvino_raw = []\n",
    "peregrine_raw = []\n",
    "sirpolitick_raw = []\n",
    "bonario_raw = []\n",
    "ladywouldbe_raw = []\n",
    "for par in token_paragraphs_votf:\n",
    "    if  'VOLP' in par:\n",
    "         volpone_raw.append(par)\n",
    "    if  'MOS' in par:\n",
    "         mosca_raw.append(par)\n",
    "    if  'NAN' in par:\n",
    "         nano_raw.append(par)\n",
    "    if  'AND' in par and ':' in par[0:2]:\n",
    "         androgyno_raw.append(par)\n",
    "    if  'VOLT' in par:\n",
    "         voltore_raw.append(par)\n",
    "    if  'CORB' in par:\n",
    "         corbaccio_raw.append(par)\n",
    "    if  'CORV' in par:\n",
    "         corvino_raw.append(par)\n",
    "    if  'PER' in par:\n",
    "         peregrine_raw.append(par)\n",
    "    if  'SIR' in par and 'P' in par:\n",
    "         sirpolitick_raw.append(par)\n",
    "    if  'BON' in par:\n",
    "         bonario_raw.append(par)\n",
    "    if  'LADY' in par and 'P' in par:\n",
    "         ladywouldbe_raw.append(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "volpone = character_cleaner(volpone_raw, 1)\n",
    "mosca = character_cleaner(mosca_raw, 2)\n",
    "nano = character_cleaner(nano_raw, 2)\n",
    "androgyno = character_cleaner(androgyno_raw, 2)\n",
    "voltore = character_cleaner(voltore_raw, 2)\n",
    "corbaccio = character_cleaner(corbaccio_raw, 2)\n",
    "corvino = character_cleaner(corvino_raw, 2)\n",
    "peregrine = character_cleaner(peregrine_raw, 2)\n",
    "sirpolitick = character_cleaner(sirpolitick_raw, 3)\n",
    "bonario = character_cleaner(bonario_raw, 2)\n",
    "ladywouldbe = character_cleaner(ladywouldbe_raw, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(444, 498, 33, 12, 113, 136)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(volpone), len(mosca), len(nano), len(androgyno), len(voltore), len(corbaccio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148, 148, 38, 108)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(peregrine), len(sirpolitick), len(bonario), len(ladywouldbe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Alchemist - Ben Jonson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_ta = paragraphs_cleaner(\"the_alchemist_jonson.txt\", squared_braquets, nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_paragraphs_ta = token_paragraphs_ta[97:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_raw = []\n",
    "subtle_raw = []\n",
    "dol_raw = []\n",
    "dapper_raw = []\n",
    "drugger_raw = []\n",
    "mammon_raw = []\n",
    "surly_raw = []\n",
    "ananias_raw = []\n",
    "tribulation_raw = []\n",
    "kastril_raw = []\n",
    "for par in token_paragraphs_ta:\n",
    "    if  'FACE' in par:\n",
    "         face_raw.append(par)\n",
    "    if  'SUB' in par:\n",
    "         subtle_raw.append(par)\n",
    "    if  'DOL' in par:\n",
    "         dol_raw.append(par)\n",
    "    if  'DAP' in par:\n",
    "         dapper_raw.append(par)\n",
    "    if  'DRUG' in par:\n",
    "         drugger_raw.append(par)\n",
    "    if  'MAM' in par:\n",
    "         mammon_raw.append(par)\n",
    "    if  'SUR' in par:\n",
    "         surly_raw.append(par)\n",
    "    if  'ANA' in par:\n",
    "         ananias_raw.append(par)\n",
    "    if  'TRI' in par:\n",
    "         tribulation_raw.append(par)\n",
    "    if  'KAS' in par:\n",
    "         kastril_raw.append(par)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "face = character_cleaner(face_raw, 2)\n",
    "subtle = character_cleaner(subtle_raw, 2)\n",
    "dol = character_cleaner(dol_raw, 2)\n",
    "dapper = character_cleaner(dapper_raw, 2)\n",
    "drugger = character_cleaner(drugger_raw, 2)\n",
    "mammon = character_cleaner(mammon_raw, 2)\n",
    "surly = character_cleaner(surly_raw, 2)\n",
    "ananias = character_cleaner(ananias_raw, 2)\n",
    "tribulation = character_cleaner(tribulation_raw, 2)\n",
    "kastril = character_cleaner(kastril_raw, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(815, 669, 143, 86, 44, 276)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(face), len(subtle), len(dol), len(dapper), len(drugger), len(mammon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139, 64, 45, 110)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(surly), len(ananias), len(tribulation), len(kastril)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
