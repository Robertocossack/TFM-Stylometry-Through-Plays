{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BURROWS' DELTA METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer works as an instance, which will tokenize a string into words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It recovers the previously stored dicts\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows to visuallize all colomns in a data frame\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data modeling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition (char_sens, n):\n",
    "    '''Splits a list of sentences into \n",
    "    n parts, of equall, or close to equal\n",
    "    number of sentences'''\n",
    "    return [char_sens[i::n] for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_split (play_characters, n):\n",
    "    '''Takes a dict as input and performs n partitions,\n",
    "    as list of sentences, in every value for every key'''\n",
    "    play_characters_part = {}\n",
    "    for char in play_characters.keys():\n",
    "        play_characters_part[char] = partition(play_characters[char], n)\n",
    "    return play_characters_part  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_partitions (play_part):\n",
    "    '''It takes a 5 partition splitted dict, and returns 5 dicts with train sets \n",
    "    as values, gathering 4 out of 5 parts everytime. It always excludes a different \n",
    "    ungathered partition for testing'''\n",
    "    part_1 = {}\n",
    "    part_2 = {}\n",
    "    part_3 = {}\n",
    "    part_4 = {}\n",
    "    part_5 = {}\n",
    "    for char in play_part:\n",
    "        for part in char:\n",
    "            part_1[char] = play_part[char][0] + play_part[char][2] + play_part[char][3] + play_part[char][4]\n",
    "            part_2[char] = play_part[char][0] + play_part[char][1] + play_part[char][3] + play_part[char][4]\n",
    "            part_3[char] = play_part[char][0] + play_part[char][1] + play_part[char][2] + play_part[char][4]\n",
    "            part_4[char] = play_part[char][0] + play_part[char][1] + play_part[char][2] + play_part[char][3]\n",
    "            part_5[char] = play_part[char][1] + play_part[char][2] + play_part[char][3] + play_part[char][4]\n",
    "    return part_1, part_2, part_3, part_4, part_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test (play_part):\n",
    "    '''It takes a 5 partition splitted dict, and returns a dict where the values are those\n",
    "    partitions which where excluded as part of the training sets'''\n",
    "    test_1 = {}\n",
    "    test_2 = {}\n",
    "    test_3 = {}\n",
    "    test_4 = {}\n",
    "    test_5 = {}\n",
    "    for char in play_part:\n",
    "        for part in char:\n",
    "            test_1[char] = play_part[char][1]\n",
    "            test_2[char] = play_part[char][2]\n",
    "            test_3[char] = play_part[char][3]\n",
    "            test_4[char] = play_part[char][4]\n",
    "            test_5[char] = play_part[char][0]\n",
    "    return test_1, test_2, test_3, test_4, test_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tok_no_punct (char):\n",
    "    '''It takes characters sentnces and returns them in lower\n",
    "    case and tokenized by words'''\n",
    "    result = []\n",
    "    for sen in char:\n",
    "        sen = sen.lower()\n",
    "        output = tokenizer.tokenize(sen)\n",
    "        result.append(output)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_unifier (character):\n",
    "    '''It joins all the words of all \n",
    "    sentences into a string'''\n",
    "    output = []\n",
    "    for sen in character:\n",
    "        for word in sen:\n",
    "            output.append(word)\n",
    "    one_string = \" \".join(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_word_tokenizer (play_characters):\n",
    "    '''It takes a characters dict as input and returns \n",
    "    another dict as result, after applying word_tok_no_punct, \n",
    "    and snetneces_unifier functions'''\n",
    "    chars = list(play_characters.keys())\n",
    "    char_tokens = {}\n",
    "    for char in chars:\n",
    "        output = word_tok_no_punct(play_characters[char])\n",
    "        result = sentences_unifier(output)\n",
    "        char_tokens[char] = result\n",
    "    return char_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_tokenizer (play_characters, n):\n",
    "    '''It takes a dict, and return another dict,\n",
    "    where the returned values are lists of \n",
    "    tokenized tuples of n words'''\n",
    "    chars = list(play_characters.keys())\n",
    "    char_ngrams_tokens = {}\n",
    "    for char in chars:\n",
    "        output = ngrams(play_characters[char],n)\n",
    "        result = list(output)\n",
    "        char_ngrams_tokens[char] = result\n",
    "    return char_ngrams_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_corpus_generator (partition):\n",
    "    '''It takes one partition in a dict from,\n",
    "    and unifies all speech lines from all \n",
    "    characters into a one unified single corpus'''\n",
    "    whole_corpus = []\n",
    "    for char in partition.keys():\n",
    "        for word in partition[char]:\n",
    "            whole_corpus.append(word)\n",
    "    return whole_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_generator (whole_corpus_freq, partition):\n",
    "    '''It takes a list of tuples, in the form of term and frequency, or tuple \n",
    "    of terms (in the case of multiple n grams) and frecuency, and the partition\n",
    "    from which it comes from as second input argument. It returns a dict with \n",
    "    characters as keys and a subordinated dict as value, where keys are the features\n",
    "    previously extracted from the whole corpus, and values its frequency of appearance\n",
    "    along the character subcorpus level'''\n",
    "    features = [word for word,freq in whole_corpus_freq]\n",
    "    feature_freqs = {}\n",
    "    for char in partition:\n",
    "        feature_freqs[char] = {} \n",
    "        overall = len(partition[char])\n",
    "        for feature in features:\n",
    "            presence = partition[char].count(feature)\n",
    "            feature_freqs[char][feature] = presence / overall\n",
    "    return  feature_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscores (df):\n",
    "    '''Converts the feature frequencies into z scores,\n",
    "    geting them out of the frequencies columns, and geting\n",
    "    rid of the latter ones afterwards'''\n",
    "    cols = list(df.columns)\n",
    "    for col in cols:\n",
    "        if type(col) == tuple:\n",
    "            join_col = col[0] + '-' + col[1]\n",
    "            col_zscore = join_col + '_zscore'\n",
    "        else:        \n",
    "            col_zscore = col + '_zscore'\n",
    "        df[col_zscore] = (df[col] - df[col].mean())/df[col].std(ddof=0)\n",
    "    df = df.drop(cols, axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is included into the get_deltas function\n",
    "def delta_distance (play_characters, test_zscores, part_zscores, character):    \n",
    "    '''It gets the delta distances between one character vector, \n",
    "    in the form of test z scores, and those corresponding to all \n",
    "    characters training partition individually'''\n",
    "    chars = play_characters.keys()\n",
    "    delta = {}\n",
    "    for char in chars:\n",
    "        delta[char] = (abs(test_zscores.loc[character] - part_zscores.loc[char])).sum()/50\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deltas (writer_characters, test_zscores, train_zscores):\n",
    "    '''It generates a data frame with characters as rows and columns,\n",
    "    where the intersection is the delta distance between them'''\n",
    "    deltas = {}\n",
    "    for char in writer_characters.keys():\n",
    "        result = delta_distance(writer_characters, test_zscores, train_zscores, char)\n",
    "        deltas[char]= result\n",
    "        df = pd.DataFrame.from_dict(deltas)\n",
    "        df = df.reindex(sorted(df.columns), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function was developed as an automatized heuristic path for the ultherior n grams analysis\n",
    "def model_predictions (plays_data, plays_test, n, plays_characters, column_name):\n",
    "    '''It takes as argumentes the trainig partition, the test partition,\n",
    "    n as the number of extracted features, the original dict, and a string \n",
    "    with the name of the resulting data frame column'''\n",
    "    plays_data_corpus = whole_corpus_generator(plays_data)\n",
    "    plays_data_corpus_freq = list(nltk.FreqDist(plays_data_corpus).most_common(n))\n",
    "    plays_data_features = features_generator(plays_data_corpus_freq, plays_data)\n",
    "    df_plays_data = pd.DataFrame.from_dict(plays_data_features, orient = 'index')\n",
    "    plays_data_zscores = zscores(df_plays_data)\n",
    "    plays_test_features = features_generator(plays_data_corpus_freq, plays_test)\n",
    "    df_plays_test = pd.DataFrame.from_dict(plays_test_features, orient = 'index')\n",
    "    plays_test_zscores = zscores(df_plays_test)\n",
    "    deltas = get_deltas(plays_characters, plays_test_zscores, plays_data_zscores)\n",
    "    predictions = deltas.idxmin()\n",
    "    df_predictions = pd.DataFrame(predictions, columns = [column_name])\n",
    "    \n",
    "    return df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def success_rate (author_results):\n",
    "    ''' It creates a column in the whole partitions predictions\n",
    "    data frame, with the character accuracy scores as result'''\n",
    "    result = []\n",
    "    for char in author_results.index:\n",
    "        output = sum(list(author_results.loc[char] == char))/5\n",
    "        result.append(output)\n",
    "    author_results['Results'] = result\n",
    "    \n",
    "    return author_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Plays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oscar Wilde "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characters (+1500 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering all plays characters into one dict\n",
    "wilde_characters = {**an_ideal_husband_characters, **a_woman_of_no_importance_characters, **lady_windermeres_fan_characters, **the_importance_of_being_earnest_characters}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wilde_characters.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split partitions generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_characters_split = corpus_split(wilde_characters, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_characters_partition = split_partitions(wilde_characters_split)\n",
    "wilde_characters_test = split_test(wilde_characters_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and test corpuses generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Original Deltha Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_part_1 = dict_word_tokenizer(wilde_characters_partition[0])\n",
    "wilde_part_2 = dict_word_tokenizer(wilde_characters_partition[1])\n",
    "wilde_part_3 = dict_word_tokenizer(wilde_characters_partition[2])\n",
    "wilde_part_4 = dict_word_tokenizer(wilde_characters_partition[3])\n",
    "wilde_part_5 = dict_word_tokenizer(wilde_characters_partition[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_test_1 = dict_word_tokenizer(wilde_characters_test[0])\n",
    "wilde_test_2 = dict_word_tokenizer(wilde_characters_test[1])\n",
    "wilde_test_3 = dict_word_tokenizer(wilde_characters_test[2])\n",
    "wilde_test_4 = dict_word_tokenizer(wilde_characters_test[3])\n",
    "wilde_test_5 = dict_word_tokenizer(wilde_characters_test[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Deltha Methd with word bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_bigrams_part_1 = ngrams_tokenizer(dict_word_tokenizer(wilde_characters_partition[0]), 2)\n",
    "wilde_bigrams_part_2 = ngrams_tokenizer(dict_word_tokenizer(wilde_characters_partition[1]), 2)\n",
    "wilde_bigrams_part_3 = ngrams_tokenizer(dict_word_tokenizer(wilde_characters_partition[2]), 2)\n",
    "wilde_bigrams_part_4 = ngrams_tokenizer(dict_word_tokenizer(wilde_characters_partition[3]), 2)\n",
    "wilde_bigrams_part_5 = ngrams_tokenizer(dict_word_tokenizer(wilde_characters_partition[4]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_bigrams_test_1 = ngrams_tokenizer(dict_word_tokenizer(wilde_characters_test[0]), 2)\n",
    "wilde_bigrams_test_2 = ngrams_tokenizer(dict_word_tokenizer(wilde_characters_test[1]), 2)\n",
    "wilde_bigrams_test_3 = ngrams_tokenizer(dict_word_tokenizer(wilde_characters_test[2]), 2)\n",
    "wilde_bigrams_test_4 = ngrams_tokenizer(dict_word_tokenizer(wilde_characters_test[3]), 2)\n",
    "wilde_bigrams_test_5 = ngrams_tokenizer(dict_word_tokenizer(wilde_characters_test[4]), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 7 characters (+4000 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geting the top 7 characters, over 4000 words each\n",
    "wilde_top_7_characters = ['Goring', 'Chiltern', 'Cheveley', 'Illingorth', 'Lady Windermere', 'Algernon', 'Jack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a dict of the top 7 characters\n",
    "wilde_top_7 = {}\n",
    "for key in wilde_characters:\n",
    "    if key in wilde_top_7_characters:\n",
    "        wilde_top_7[key] = wilde_characters[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Goring', 'Chiltern', 'Cheveley', 'Illingorth', 'Lady Windermere', 'Jack', 'Algernon'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wilde_top_7.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'wilde_top_7' (dict)\n"
     ]
    }
   ],
   "source": [
    "%store wilde_top_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_part_1_corpus = whole_corpus_generator(wilde_part_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the top 50 frequency terms from the whole partition corpus\n",
    "wilde_part_1_corpus_freq = list(nltk.FreqDist(wilde_part_1_corpus).most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_part_1_features = features_generator(wilde_part_1_corpus_freq, wilde_part_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make a pandas Data Frame from the resulting dict of dicts\n",
    "df_wilde_1 = pd.DataFrame.from_dict(wilde_part_1_features, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the training z scores of the first partition in a Data Frame\n",
    "wilde_zscores_1 = zscores(df_wilde_1)\n",
    "wilde_zscores_1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We repeat the same process to generate the test z scores\n",
    "wilde_test_1_features = features_generator(wilde_part_1_corpus_freq, wilde_test_1)\n",
    "df_wilde_test_1 = pd.DataFrame.from_dict(wilde_test_1_features, orient = 'index')\n",
    "wilde_zscores_test_1 = zscores(df_wilde_test_1)\n",
    "wilde_zscores_test_1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algernon</th>\n",
       "      <th>Allonby</th>\n",
       "      <th>Berwick</th>\n",
       "      <th>Bracknell</th>\n",
       "      <th>Caversham</th>\n",
       "      <th>Cecily</th>\n",
       "      <th>Cheveley</th>\n",
       "      <th>Chiltern</th>\n",
       "      <th>Darlington</th>\n",
       "      <th>Erlynne</th>\n",
       "      <th>Gerald</th>\n",
       "      <th>Goring</th>\n",
       "      <th>Gwendolen</th>\n",
       "      <th>Hunstanton</th>\n",
       "      <th>Illingorth</th>\n",
       "      <th>Jack</th>\n",
       "      <th>Lady Chiltern</th>\n",
       "      <th>Lady Windermere</th>\n",
       "      <th>Lord Windermere</th>\n",
       "      <th>Mabel</th>\n",
       "      <th>Mrs Artbuthnot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Caversham</th>\n",
       "      <td>1.033737</td>\n",
       "      <td>1.430973</td>\n",
       "      <td>1.367975</td>\n",
       "      <td>1.382483</td>\n",
       "      <td>1.342596</td>\n",
       "      <td>1.101039</td>\n",
       "      <td>1.044462</td>\n",
       "      <td>1.135720</td>\n",
       "      <td>1.254766</td>\n",
       "      <td>1.101859</td>\n",
       "      <td>1.408035</td>\n",
       "      <td>1.094243</td>\n",
       "      <td>1.292960</td>\n",
       "      <td>1.205952</td>\n",
       "      <td>1.284633</td>\n",
       "      <td>1.215895</td>\n",
       "      <td>1.330462</td>\n",
       "      <td>1.065195</td>\n",
       "      <td>1.445795</td>\n",
       "      <td>1.116541</td>\n",
       "      <td>1.463528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Goring</th>\n",
       "      <td>0.721299</td>\n",
       "      <td>0.853645</td>\n",
       "      <td>0.892126</td>\n",
       "      <td>0.943398</td>\n",
       "      <td>1.120469</td>\n",
       "      <td>0.877074</td>\n",
       "      <td>0.708305</td>\n",
       "      <td>0.752967</td>\n",
       "      <td>1.021297</td>\n",
       "      <td>0.955769</td>\n",
       "      <td>1.109533</td>\n",
       "      <td>0.586983</td>\n",
       "      <td>0.891591</td>\n",
       "      <td>0.870076</td>\n",
       "      <td>0.763833</td>\n",
       "      <td>0.744007</td>\n",
       "      <td>0.919483</td>\n",
       "      <td>0.678023</td>\n",
       "      <td>1.339162</td>\n",
       "      <td>1.118264</td>\n",
       "      <td>0.944648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chiltern</th>\n",
       "      <td>0.889077</td>\n",
       "      <td>1.076256</td>\n",
       "      <td>1.127302</td>\n",
       "      <td>1.042658</td>\n",
       "      <td>1.303227</td>\n",
       "      <td>0.990817</td>\n",
       "      <td>0.807185</td>\n",
       "      <td>0.609886</td>\n",
       "      <td>1.134823</td>\n",
       "      <td>1.105166</td>\n",
       "      <td>1.127037</td>\n",
       "      <td>0.814958</td>\n",
       "      <td>0.837212</td>\n",
       "      <td>1.051107</td>\n",
       "      <td>0.985698</td>\n",
       "      <td>0.834587</td>\n",
       "      <td>0.967572</td>\n",
       "      <td>0.880793</td>\n",
       "      <td>1.367009</td>\n",
       "      <td>1.187462</td>\n",
       "      <td>1.090397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lady Chiltern</th>\n",
       "      <td>1.053716</td>\n",
       "      <td>1.169819</td>\n",
       "      <td>1.231378</td>\n",
       "      <td>1.381290</td>\n",
       "      <td>1.249683</td>\n",
       "      <td>1.065979</td>\n",
       "      <td>0.870374</td>\n",
       "      <td>0.952331</td>\n",
       "      <td>1.099986</td>\n",
       "      <td>1.099480</td>\n",
       "      <td>1.113581</td>\n",
       "      <td>0.944472</td>\n",
       "      <td>1.016097</td>\n",
       "      <td>1.169280</td>\n",
       "      <td>1.140046</td>\n",
       "      <td>0.922919</td>\n",
       "      <td>0.914569</td>\n",
       "      <td>0.876895</td>\n",
       "      <td>1.255746</td>\n",
       "      <td>1.298012</td>\n",
       "      <td>1.002711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mabel</th>\n",
       "      <td>1.071769</td>\n",
       "      <td>0.948101</td>\n",
       "      <td>1.098839</td>\n",
       "      <td>1.179960</td>\n",
       "      <td>1.479255</td>\n",
       "      <td>1.145005</td>\n",
       "      <td>0.841133</td>\n",
       "      <td>1.021109</td>\n",
       "      <td>1.174214</td>\n",
       "      <td>0.967520</td>\n",
       "      <td>1.353732</td>\n",
       "      <td>1.056616</td>\n",
       "      <td>1.042822</td>\n",
       "      <td>1.063938</td>\n",
       "      <td>1.018370</td>\n",
       "      <td>1.023509</td>\n",
       "      <td>1.149039</td>\n",
       "      <td>1.094309</td>\n",
       "      <td>1.607605</td>\n",
       "      <td>1.066375</td>\n",
       "      <td>1.400244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cheveley</th>\n",
       "      <td>0.759770</td>\n",
       "      <td>1.110198</td>\n",
       "      <td>1.081334</td>\n",
       "      <td>1.113360</td>\n",
       "      <td>1.129525</td>\n",
       "      <td>0.949892</td>\n",
       "      <td>0.649698</td>\n",
       "      <td>0.835915</td>\n",
       "      <td>1.137351</td>\n",
       "      <td>0.984681</td>\n",
       "      <td>1.381887</td>\n",
       "      <td>0.789738</td>\n",
       "      <td>0.823695</td>\n",
       "      <td>0.997054</td>\n",
       "      <td>0.913406</td>\n",
       "      <td>0.876684</td>\n",
       "      <td>1.041562</td>\n",
       "      <td>0.931183</td>\n",
       "      <td>1.473596</td>\n",
       "      <td>0.934219</td>\n",
       "      <td>1.032912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Illingorth</th>\n",
       "      <td>0.794116</td>\n",
       "      <td>1.058433</td>\n",
       "      <td>1.111503</td>\n",
       "      <td>1.120901</td>\n",
       "      <td>1.212437</td>\n",
       "      <td>0.900600</td>\n",
       "      <td>0.837670</td>\n",
       "      <td>0.888472</td>\n",
       "      <td>1.250889</td>\n",
       "      <td>1.155778</td>\n",
       "      <td>1.299178</td>\n",
       "      <td>0.561114</td>\n",
       "      <td>1.069462</td>\n",
       "      <td>0.903449</td>\n",
       "      <td>0.731807</td>\n",
       "      <td>0.924919</td>\n",
       "      <td>1.195525</td>\n",
       "      <td>0.881236</td>\n",
       "      <td>1.553490</td>\n",
       "      <td>1.265370</td>\n",
       "      <td>1.239962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Allonby</th>\n",
       "      <td>1.197143</td>\n",
       "      <td>1.182088</td>\n",
       "      <td>1.321777</td>\n",
       "      <td>1.247430</td>\n",
       "      <td>1.457315</td>\n",
       "      <td>1.367638</td>\n",
       "      <td>1.089348</td>\n",
       "      <td>1.202147</td>\n",
       "      <td>1.306406</td>\n",
       "      <td>1.502052</td>\n",
       "      <td>1.350104</td>\n",
       "      <td>1.178447</td>\n",
       "      <td>1.330902</td>\n",
       "      <td>1.034472</td>\n",
       "      <td>1.008233</td>\n",
       "      <td>1.301887</td>\n",
       "      <td>1.542089</td>\n",
       "      <td>1.224258</td>\n",
       "      <td>1.718433</td>\n",
       "      <td>1.573390</td>\n",
       "      <td>1.526129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gerald</th>\n",
       "      <td>1.033375</td>\n",
       "      <td>1.176916</td>\n",
       "      <td>1.291224</td>\n",
       "      <td>1.372249</td>\n",
       "      <td>1.484601</td>\n",
       "      <td>1.082839</td>\n",
       "      <td>0.979826</td>\n",
       "      <td>1.151353</td>\n",
       "      <td>1.273519</td>\n",
       "      <td>0.912077</td>\n",
       "      <td>1.085833</td>\n",
       "      <td>1.098760</td>\n",
       "      <td>1.131989</td>\n",
       "      <td>1.300497</td>\n",
       "      <td>1.062910</td>\n",
       "      <td>0.943163</td>\n",
       "      <td>1.186524</td>\n",
       "      <td>0.925176</td>\n",
       "      <td>1.406706</td>\n",
       "      <td>1.267200</td>\n",
       "      <td>1.197908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mrs Artbuthnot</th>\n",
       "      <td>1.302010</td>\n",
       "      <td>1.268215</td>\n",
       "      <td>1.396809</td>\n",
       "      <td>1.449143</td>\n",
       "      <td>1.361715</td>\n",
       "      <td>1.185858</td>\n",
       "      <td>1.115614</td>\n",
       "      <td>1.070707</td>\n",
       "      <td>1.332545</td>\n",
       "      <td>1.359566</td>\n",
       "      <td>1.279934</td>\n",
       "      <td>1.193592</td>\n",
       "      <td>1.316964</td>\n",
       "      <td>1.311042</td>\n",
       "      <td>1.202622</td>\n",
       "      <td>1.268683</td>\n",
       "      <td>1.166761</td>\n",
       "      <td>0.991426</td>\n",
       "      <td>1.226699</td>\n",
       "      <td>1.518168</td>\n",
       "      <td>0.849024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hunstanton</th>\n",
       "      <td>1.163827</td>\n",
       "      <td>1.031402</td>\n",
       "      <td>0.990854</td>\n",
       "      <td>1.289155</td>\n",
       "      <td>1.308307</td>\n",
       "      <td>1.037673</td>\n",
       "      <td>0.970901</td>\n",
       "      <td>0.959776</td>\n",
       "      <td>1.057774</td>\n",
       "      <td>1.164157</td>\n",
       "      <td>1.228765</td>\n",
       "      <td>0.961319</td>\n",
       "      <td>1.170646</td>\n",
       "      <td>0.844118</td>\n",
       "      <td>0.961267</td>\n",
       "      <td>1.158228</td>\n",
       "      <td>1.077581</td>\n",
       "      <td>1.036459</td>\n",
       "      <td>1.401721</td>\n",
       "      <td>1.239756</td>\n",
       "      <td>1.211240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lord Windermere</th>\n",
       "      <td>1.052544</td>\n",
       "      <td>1.389767</td>\n",
       "      <td>1.266048</td>\n",
       "      <td>1.351069</td>\n",
       "      <td>1.249916</td>\n",
       "      <td>1.219355</td>\n",
       "      <td>1.062066</td>\n",
       "      <td>1.125533</td>\n",
       "      <td>1.286281</td>\n",
       "      <td>1.056093</td>\n",
       "      <td>1.429622</td>\n",
       "      <td>1.056537</td>\n",
       "      <td>1.190169</td>\n",
       "      <td>1.293951</td>\n",
       "      <td>1.360889</td>\n",
       "      <td>1.087416</td>\n",
       "      <td>1.253560</td>\n",
       "      <td>0.875975</td>\n",
       "      <td>0.921754</td>\n",
       "      <td>1.354630</td>\n",
       "      <td>1.264278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Erlynne</th>\n",
       "      <td>0.997981</td>\n",
       "      <td>1.159976</td>\n",
       "      <td>1.149347</td>\n",
       "      <td>1.324865</td>\n",
       "      <td>1.333110</td>\n",
       "      <td>0.963991</td>\n",
       "      <td>0.931395</td>\n",
       "      <td>0.958457</td>\n",
       "      <td>1.204417</td>\n",
       "      <td>0.978103</td>\n",
       "      <td>1.205855</td>\n",
       "      <td>0.987726</td>\n",
       "      <td>1.041900</td>\n",
       "      <td>1.066546</td>\n",
       "      <td>1.105892</td>\n",
       "      <td>0.968854</td>\n",
       "      <td>0.971410</td>\n",
       "      <td>0.818647</td>\n",
       "      <td>1.113654</td>\n",
       "      <td>1.251503</td>\n",
       "      <td>1.126446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lady Windermere</th>\n",
       "      <td>0.955611</td>\n",
       "      <td>1.087678</td>\n",
       "      <td>1.144185</td>\n",
       "      <td>1.225744</td>\n",
       "      <td>1.284948</td>\n",
       "      <td>0.914157</td>\n",
       "      <td>0.858474</td>\n",
       "      <td>0.880376</td>\n",
       "      <td>1.085887</td>\n",
       "      <td>0.844785</td>\n",
       "      <td>1.195743</td>\n",
       "      <td>0.863264</td>\n",
       "      <td>1.053084</td>\n",
       "      <td>1.090966</td>\n",
       "      <td>1.118497</td>\n",
       "      <td>0.877157</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.659188</td>\n",
       "      <td>1.054784</td>\n",
       "      <td>1.221527</td>\n",
       "      <td>0.941280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Darlington</th>\n",
       "      <td>1.104308</td>\n",
       "      <td>1.396401</td>\n",
       "      <td>1.361843</td>\n",
       "      <td>1.360093</td>\n",
       "      <td>1.435129</td>\n",
       "      <td>0.997097</td>\n",
       "      <td>1.027987</td>\n",
       "      <td>1.217443</td>\n",
       "      <td>1.084292</td>\n",
       "      <td>1.162891</td>\n",
       "      <td>1.577129</td>\n",
       "      <td>1.064926</td>\n",
       "      <td>1.075916</td>\n",
       "      <td>1.296616</td>\n",
       "      <td>1.215286</td>\n",
       "      <td>1.283084</td>\n",
       "      <td>1.354023</td>\n",
       "      <td>1.259008</td>\n",
       "      <td>1.533662</td>\n",
       "      <td>1.378214</td>\n",
       "      <td>1.388078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Berwick</th>\n",
       "      <td>1.318849</td>\n",
       "      <td>1.060492</td>\n",
       "      <td>0.862000</td>\n",
       "      <td>1.313268</td>\n",
       "      <td>1.353224</td>\n",
       "      <td>1.403589</td>\n",
       "      <td>1.151617</td>\n",
       "      <td>1.200378</td>\n",
       "      <td>1.129765</td>\n",
       "      <td>1.334440</td>\n",
       "      <td>1.460507</td>\n",
       "      <td>1.236428</td>\n",
       "      <td>1.386546</td>\n",
       "      <td>1.133946</td>\n",
       "      <td>1.178533</td>\n",
       "      <td>1.373012</td>\n",
       "      <td>1.257741</td>\n",
       "      <td>1.187833</td>\n",
       "      <td>1.573574</td>\n",
       "      <td>1.554187</td>\n",
       "      <td>1.390848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jack</th>\n",
       "      <td>0.705186</td>\n",
       "      <td>1.010328</td>\n",
       "      <td>0.911645</td>\n",
       "      <td>1.113024</td>\n",
       "      <td>1.295462</td>\n",
       "      <td>0.901903</td>\n",
       "      <td>0.758730</td>\n",
       "      <td>0.806497</td>\n",
       "      <td>1.089348</td>\n",
       "      <td>1.048210</td>\n",
       "      <td>1.193068</td>\n",
       "      <td>0.781614</td>\n",
       "      <td>0.875093</td>\n",
       "      <td>0.909996</td>\n",
       "      <td>1.043047</td>\n",
       "      <td>0.725510</td>\n",
       "      <td>1.063495</td>\n",
       "      <td>0.924871</td>\n",
       "      <td>1.461196</td>\n",
       "      <td>1.193720</td>\n",
       "      <td>1.058445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algernon</th>\n",
       "      <td>0.781864</td>\n",
       "      <td>1.095510</td>\n",
       "      <td>1.142118</td>\n",
       "      <td>1.173423</td>\n",
       "      <td>1.273716</td>\n",
       "      <td>0.800433</td>\n",
       "      <td>0.853107</td>\n",
       "      <td>0.873166</td>\n",
       "      <td>1.147787</td>\n",
       "      <td>0.961320</td>\n",
       "      <td>1.156642</td>\n",
       "      <td>0.880044</td>\n",
       "      <td>0.975575</td>\n",
       "      <td>0.964585</td>\n",
       "      <td>1.016151</td>\n",
       "      <td>0.804512</td>\n",
       "      <td>1.096616</td>\n",
       "      <td>0.947973</td>\n",
       "      <td>1.477402</td>\n",
       "      <td>1.187546</td>\n",
       "      <td>1.301376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gwendolen</th>\n",
       "      <td>1.034730</td>\n",
       "      <td>1.302634</td>\n",
       "      <td>1.359999</td>\n",
       "      <td>1.190685</td>\n",
       "      <td>1.255842</td>\n",
       "      <td>0.987533</td>\n",
       "      <td>0.897139</td>\n",
       "      <td>0.973403</td>\n",
       "      <td>1.104664</td>\n",
       "      <td>1.167032</td>\n",
       "      <td>1.397578</td>\n",
       "      <td>0.833521</td>\n",
       "      <td>1.028928</td>\n",
       "      <td>1.289725</td>\n",
       "      <td>1.202047</td>\n",
       "      <td>0.981748</td>\n",
       "      <td>1.042760</td>\n",
       "      <td>1.121992</td>\n",
       "      <td>1.587879</td>\n",
       "      <td>1.133959</td>\n",
       "      <td>1.160934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bracknell</th>\n",
       "      <td>0.967906</td>\n",
       "      <td>1.284578</td>\n",
       "      <td>1.128378</td>\n",
       "      <td>0.809430</td>\n",
       "      <td>1.140733</td>\n",
       "      <td>0.958113</td>\n",
       "      <td>0.874259</td>\n",
       "      <td>0.964968</td>\n",
       "      <td>1.341725</td>\n",
       "      <td>1.270020</td>\n",
       "      <td>1.414406</td>\n",
       "      <td>0.901593</td>\n",
       "      <td>1.033596</td>\n",
       "      <td>0.864285</td>\n",
       "      <td>1.074308</td>\n",
       "      <td>0.938061</td>\n",
       "      <td>1.064372</td>\n",
       "      <td>1.086987</td>\n",
       "      <td>1.607722</td>\n",
       "      <td>1.130536</td>\n",
       "      <td>1.283479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cecily</th>\n",
       "      <td>0.849030</td>\n",
       "      <td>0.982371</td>\n",
       "      <td>1.121274</td>\n",
       "      <td>1.190338</td>\n",
       "      <td>1.300687</td>\n",
       "      <td>0.883999</td>\n",
       "      <td>0.852804</td>\n",
       "      <td>0.870319</td>\n",
       "      <td>0.963788</td>\n",
       "      <td>0.845956</td>\n",
       "      <td>1.214650</td>\n",
       "      <td>0.899635</td>\n",
       "      <td>0.829635</td>\n",
       "      <td>1.008554</td>\n",
       "      <td>0.946389</td>\n",
       "      <td>0.796540</td>\n",
       "      <td>1.038171</td>\n",
       "      <td>0.993867</td>\n",
       "      <td>1.498208</td>\n",
       "      <td>1.077023</td>\n",
       "      <td>1.125138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Algernon   Allonby   Berwick  Bracknell  Caversham    Cecily  \\\n",
       "Caversham        1.033737  1.430973  1.367975   1.382483   1.342596  1.101039   \n",
       "Goring           0.721299  0.853645  0.892126   0.943398   1.120469  0.877074   \n",
       "Chiltern         0.889077  1.076256  1.127302   1.042658   1.303227  0.990817   \n",
       "Lady Chiltern    1.053716  1.169819  1.231378   1.381290   1.249683  1.065979   \n",
       "Mabel            1.071769  0.948101  1.098839   1.179960   1.479255  1.145005   \n",
       "Cheveley         0.759770  1.110198  1.081334   1.113360   1.129525  0.949892   \n",
       "Illingorth       0.794116  1.058433  1.111503   1.120901   1.212437  0.900600   \n",
       "Allonby          1.197143  1.182088  1.321777   1.247430   1.457315  1.367638   \n",
       "Gerald           1.033375  1.176916  1.291224   1.372249   1.484601  1.082839   \n",
       "Mrs Artbuthnot   1.302010  1.268215  1.396809   1.449143   1.361715  1.185858   \n",
       "Hunstanton       1.163827  1.031402  0.990854   1.289155   1.308307  1.037673   \n",
       "Lord Windermere  1.052544  1.389767  1.266048   1.351069   1.249916  1.219355   \n",
       "Erlynne          0.997981  1.159976  1.149347   1.324865   1.333110  0.963991   \n",
       "Lady Windermere  0.955611  1.087678  1.144185   1.225744   1.284948  0.914157   \n",
       "Darlington       1.104308  1.396401  1.361843   1.360093   1.435129  0.997097   \n",
       "Berwick          1.318849  1.060492  0.862000   1.313268   1.353224  1.403589   \n",
       "Jack             0.705186  1.010328  0.911645   1.113024   1.295462  0.901903   \n",
       "Algernon         0.781864  1.095510  1.142118   1.173423   1.273716  0.800433   \n",
       "Gwendolen        1.034730  1.302634  1.359999   1.190685   1.255842  0.987533   \n",
       "Bracknell        0.967906  1.284578  1.128378   0.809430   1.140733  0.958113   \n",
       "Cecily           0.849030  0.982371  1.121274   1.190338   1.300687  0.883999   \n",
       "\n",
       "                 Cheveley  Chiltern  Darlington   Erlynne    Gerald    Goring  \\\n",
       "Caversham        1.044462  1.135720    1.254766  1.101859  1.408035  1.094243   \n",
       "Goring           0.708305  0.752967    1.021297  0.955769  1.109533  0.586983   \n",
       "Chiltern         0.807185  0.609886    1.134823  1.105166  1.127037  0.814958   \n",
       "Lady Chiltern    0.870374  0.952331    1.099986  1.099480  1.113581  0.944472   \n",
       "Mabel            0.841133  1.021109    1.174214  0.967520  1.353732  1.056616   \n",
       "Cheveley         0.649698  0.835915    1.137351  0.984681  1.381887  0.789738   \n",
       "Illingorth       0.837670  0.888472    1.250889  1.155778  1.299178  0.561114   \n",
       "Allonby          1.089348  1.202147    1.306406  1.502052  1.350104  1.178447   \n",
       "Gerald           0.979826  1.151353    1.273519  0.912077  1.085833  1.098760   \n",
       "Mrs Artbuthnot   1.115614  1.070707    1.332545  1.359566  1.279934  1.193592   \n",
       "Hunstanton       0.970901  0.959776    1.057774  1.164157  1.228765  0.961319   \n",
       "Lord Windermere  1.062066  1.125533    1.286281  1.056093  1.429622  1.056537   \n",
       "Erlynne          0.931395  0.958457    1.204417  0.978103  1.205855  0.987726   \n",
       "Lady Windermere  0.858474  0.880376    1.085887  0.844785  1.195743  0.863264   \n",
       "Darlington       1.027987  1.217443    1.084292  1.162891  1.577129  1.064926   \n",
       "Berwick          1.151617  1.200378    1.129765  1.334440  1.460507  1.236428   \n",
       "Jack             0.758730  0.806497    1.089348  1.048210  1.193068  0.781614   \n",
       "Algernon         0.853107  0.873166    1.147787  0.961320  1.156642  0.880044   \n",
       "Gwendolen        0.897139  0.973403    1.104664  1.167032  1.397578  0.833521   \n",
       "Bracknell        0.874259  0.964968    1.341725  1.270020  1.414406  0.901593   \n",
       "Cecily           0.852804  0.870319    0.963788  0.845956  1.214650  0.899635   \n",
       "\n",
       "                 Gwendolen  Hunstanton  Illingorth      Jack  Lady Chiltern  \\\n",
       "Caversham         1.292960    1.205952    1.284633  1.215895       1.330462   \n",
       "Goring            0.891591    0.870076    0.763833  0.744007       0.919483   \n",
       "Chiltern          0.837212    1.051107    0.985698  0.834587       0.967572   \n",
       "Lady Chiltern     1.016097    1.169280    1.140046  0.922919       0.914569   \n",
       "Mabel             1.042822    1.063938    1.018370  1.023509       1.149039   \n",
       "Cheveley          0.823695    0.997054    0.913406  0.876684       1.041562   \n",
       "Illingorth        1.069462    0.903449    0.731807  0.924919       1.195525   \n",
       "Allonby           1.330902    1.034472    1.008233  1.301887       1.542089   \n",
       "Gerald            1.131989    1.300497    1.062910  0.943163       1.186524   \n",
       "Mrs Artbuthnot    1.316964    1.311042    1.202622  1.268683       1.166761   \n",
       "Hunstanton        1.170646    0.844118    0.961267  1.158228       1.077581   \n",
       "Lord Windermere   1.190169    1.293951    1.360889  1.087416       1.253560   \n",
       "Erlynne           1.041900    1.066546    1.105892  0.968854       0.971410   \n",
       "Lady Windermere   1.053084    1.090966    1.118497  0.877157       0.930000   \n",
       "Darlington        1.075916    1.296616    1.215286  1.283084       1.354023   \n",
       "Berwick           1.386546    1.133946    1.178533  1.373012       1.257741   \n",
       "Jack              0.875093    0.909996    1.043047  0.725510       1.063495   \n",
       "Algernon          0.975575    0.964585    1.016151  0.804512       1.096616   \n",
       "Gwendolen         1.028928    1.289725    1.202047  0.981748       1.042760   \n",
       "Bracknell         1.033596    0.864285    1.074308  0.938061       1.064372   \n",
       "Cecily            0.829635    1.008554    0.946389  0.796540       1.038171   \n",
       "\n",
       "                 Lady Windermere  Lord Windermere     Mabel  Mrs Artbuthnot  \n",
       "Caversham               1.065195         1.445795  1.116541        1.463528  \n",
       "Goring                  0.678023         1.339162  1.118264        0.944648  \n",
       "Chiltern                0.880793         1.367009  1.187462        1.090397  \n",
       "Lady Chiltern           0.876895         1.255746  1.298012        1.002711  \n",
       "Mabel                   1.094309         1.607605  1.066375        1.400244  \n",
       "Cheveley                0.931183         1.473596  0.934219        1.032912  \n",
       "Illingorth              0.881236         1.553490  1.265370        1.239962  \n",
       "Allonby                 1.224258         1.718433  1.573390        1.526129  \n",
       "Gerald                  0.925176         1.406706  1.267200        1.197908  \n",
       "Mrs Artbuthnot          0.991426         1.226699  1.518168        0.849024  \n",
       "Hunstanton              1.036459         1.401721  1.239756        1.211240  \n",
       "Lord Windermere         0.875975         0.921754  1.354630        1.264278  \n",
       "Erlynne                 0.818647         1.113654  1.251503        1.126446  \n",
       "Lady Windermere         0.659188         1.054784  1.221527        0.941280  \n",
       "Darlington              1.259008         1.533662  1.378214        1.388078  \n",
       "Berwick                 1.187833         1.573574  1.554187        1.390848  \n",
       "Jack                    0.924871         1.461196  1.193720        1.058445  \n",
       "Algernon                0.947973         1.477402  1.187546        1.301376  \n",
       "Gwendolen               1.121992         1.587879  1.133959        1.160934  \n",
       "Bracknell               1.086987         1.607722  1.130536        1.283479  \n",
       "Cecily                  0.993867         1.498208  1.077023        1.125138  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wilde_1_deltas = get_deltas(wilde_characters, wilde_zscores_test_1, wilde_zscores_1)\n",
    "wilde_1_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the prediction from the closest distance match\n",
    "predictions_wilde_1 = wilde_1_deltas.idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Part_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Algernon</th>\n",
       "      <td>Jack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Allonby</th>\n",
       "      <td>Goring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Berwick</th>\n",
       "      <td>Berwick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bracknell</th>\n",
       "      <td>Bracknell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Caversham</th>\n",
       "      <td>Goring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cecily</th>\n",
       "      <td>Algernon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cheveley</th>\n",
       "      <td>Cheveley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chiltern</th>\n",
       "      <td>Chiltern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Darlington</th>\n",
       "      <td>Cecily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Erlynne</th>\n",
       "      <td>Lady Windermere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gerald</th>\n",
       "      <td>Gerald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Goring</th>\n",
       "      <td>Illingorth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gwendolen</th>\n",
       "      <td>Cheveley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hunstanton</th>\n",
       "      <td>Hunstanton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Illingorth</th>\n",
       "      <td>Illingorth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jack</th>\n",
       "      <td>Jack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lady Chiltern</th>\n",
       "      <td>Lady Chiltern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lady Windermere</th>\n",
       "      <td>Lady Windermere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lord Windermere</th>\n",
       "      <td>Lord Windermere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mabel</th>\n",
       "      <td>Cheveley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mrs Artbuthnot</th>\n",
       "      <td>Mrs Artbuthnot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Part_1\n",
       "Algernon                    Jack\n",
       "Allonby                   Goring\n",
       "Berwick                  Berwick\n",
       "Bracknell              Bracknell\n",
       "Caversham                 Goring\n",
       "Cecily                  Algernon\n",
       "Cheveley                Cheveley\n",
       "Chiltern                Chiltern\n",
       "Darlington                Cecily\n",
       "Erlynne          Lady Windermere\n",
       "Gerald                    Gerald\n",
       "Goring                Illingorth\n",
       "Gwendolen               Cheveley\n",
       "Hunstanton            Hunstanton\n",
       "Illingorth            Illingorth\n",
       "Jack                        Jack\n",
       "Lady Chiltern      Lady Chiltern\n",
       "Lady Windermere  Lady Windermere\n",
       "Lord Windermere  Lord Windermere\n",
       "Mabel                   Cheveley\n",
       "Mrs Artbuthnot    Mrs Artbuthnot"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating a data frame with the predictions of the partition as column\n",
    "wilde_df_1 = pd.DataFrame(predictions_wilde_1, columns = ['Part_1'])\n",
    "wilde_df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deltha Method bi-grams predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Part_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Algernon</th>\n",
       "      <td>Algernon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Allonby</th>\n",
       "      <td>Illingorth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Berwick</th>\n",
       "      <td>Illingorth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bracknell</th>\n",
       "      <td>Bracknell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Caversham</th>\n",
       "      <td>Illingorth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cecily</th>\n",
       "      <td>Algernon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cheveley</th>\n",
       "      <td>Lady Windermere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chiltern</th>\n",
       "      <td>Chiltern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Darlington</th>\n",
       "      <td>Darlington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Erlynne</th>\n",
       "      <td>Lady Windermere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gerald</th>\n",
       "      <td>Gerald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Goring</th>\n",
       "      <td>Goring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gwendolen</th>\n",
       "      <td>Algernon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hunstanton</th>\n",
       "      <td>Hunstanton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Illingorth</th>\n",
       "      <td>Goring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jack</th>\n",
       "      <td>Algernon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lady Chiltern</th>\n",
       "      <td>Lady Windermere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lady Windermere</th>\n",
       "      <td>Lady Windermere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lord Windermere</th>\n",
       "      <td>Lady Windermere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mabel</th>\n",
       "      <td>Mabel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mrs Artbuthnot</th>\n",
       "      <td>Mrs Artbuthnot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Part_1\n",
       "Algernon                Algernon\n",
       "Allonby               Illingorth\n",
       "Berwick               Illingorth\n",
       "Bracknell              Bracknell\n",
       "Caversham             Illingorth\n",
       "Cecily                  Algernon\n",
       "Cheveley         Lady Windermere\n",
       "Chiltern                Chiltern\n",
       "Darlington            Darlington\n",
       "Erlynne          Lady Windermere\n",
       "Gerald                    Gerald\n",
       "Goring                    Goring\n",
       "Gwendolen               Algernon\n",
       "Hunstanton            Hunstanton\n",
       "Illingorth                Goring\n",
       "Jack                    Algernon\n",
       "Lady Chiltern    Lady Windermere\n",
       "Lady Windermere  Lady Windermere\n",
       "Lord Windermere  Lady Windermere\n",
       "Mabel                      Mabel\n",
       "Mrs Artbuthnot    Mrs Artbuthnot"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wilde_bigram_df_1 = model_predictions(wilde_bigrams_part_1, wilde_bigrams_test_1, 50, wilde_characters, 'Part_1')\n",
    "wilde_bigram_df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train z- scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_part_2_corpus = whole_corpus_generator(wilde_part_2)\n",
    "wilde_part_2_corpus_freq = list(nltk.FreqDist(wilde_part_2_corpus).most_common(50))\n",
    "wilde_part_2_features = features_generator(wilde_part_2_corpus_freq, wilde_part_2)\n",
    "df_wilde_2 = pd.DataFrame.from_dict(wilde_part_2_features, orient = 'index')\n",
    "wilde_zscores_2 = zscores(df_wilde_2)\n",
    "wilde_zscores_2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_test_2_features = features_generator(wilde_part_2_corpus_freq, wilde_test_2)\n",
    "df_wilde_test_2 = pd.DataFrame.from_dict(wilde_test_2_features, orient = 'index')\n",
    "wilde_zscores_test_2 = zscores(df_wilde_test_2)\n",
    "wilde_zscores_test_2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_2_deltas = get_deltas(wilde_characters, wilde_zscores_test_2, wilde_zscores_2)\n",
    "predictions_wilde_2 = wilde_2_deltas.idxmin()\n",
    "wilde_df_2 = pd.DataFrame(predictions_wilde_2, columns = ['Part_2'])\n",
    "wilde_df_2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_bigram_df_2 = model_predictions(wilde_bigrams_part_2, wilde_bigrams_test_2, 50, wilde_characters, 'Part_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_part_3_corpus = whole_corpus_generator(wilde_part_3)\n",
    "wilde_part_3_corpus_freq = list(nltk.FreqDist(wilde_part_3_corpus).most_common(50))\n",
    "wilde_part_3_features = features_generator(wilde_part_3_corpus_freq, wilde_part_3)\n",
    "df_wilde_3 = pd.DataFrame.from_dict(wilde_part_3_features, orient = 'index')\n",
    "wilde_zscores_3 = zscores(df_wilde_3)\n",
    "wilde_zscores_3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_test_3_features = features_generator(wilde_part_3_corpus_freq, wilde_test_3)\n",
    "df_wilde_test_3 = pd.DataFrame.from_dict(wilde_test_3_features, orient = 'index')\n",
    "wilde_zscores_test_3 = zscores(df_wilde_test_3)\n",
    "wilde_zscores_test_3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_3_deltas = get_deltas(wilde_characters, wilde_zscores_test_3, wilde_zscores_3)\n",
    "predictions_wilde_3 = wilde_3_deltas.idxmin()\n",
    "wilde_df_3 = pd.DataFrame(predictions_wilde_3, columns = ['Part_3'])\n",
    "wilde_df_3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_bigram_df_3 = model_predictions(wilde_bigrams_part_3, wilde_bigrams_test_3, 50, wilde_characters, 'Part_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_part_4_corpus = whole_corpus_generator(wilde_part_4)\n",
    "wilde_part_4_corpus_freq = list(nltk.FreqDist(wilde_part_4_corpus).most_common(50))\n",
    "wilde_part_4_features = features_generator(wilde_part_4_corpus_freq, wilde_part_4)\n",
    "df_wilde_4 = pd.DataFrame.from_dict(wilde_part_4_features, orient = 'index')\n",
    "wilde_zscores_4 = zscores(df_wilde_4)\n",
    "wilde_zscores_4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_test_4_features = features_generator(wilde_part_4_corpus_freq, wilde_test_4)\n",
    "df_wilde_test_4 = pd.DataFrame.from_dict(wilde_test_4_features, orient = 'index')\n",
    "wilde_zscores_test_4 = zscores(df_wilde_test_4)\n",
    "wilde_zscores_test_4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_4_deltas = get_deltas(wilde_characters, wilde_zscores_test_4, wilde_zscores_4)\n",
    "predictions_wilde_4 = wilde_4_deltas.idxmin()\n",
    "wilde_df_4 = pd.DataFrame(predictions_wilde_4, columns = ['Part_4'])\n",
    "wilde_df_4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_bigram_df_4 = model_predictions(wilde_bigrams_part_4, wilde_bigrams_test_4, 50, wilde_characters, 'Part_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_part_5_corpus = whole_corpus_generator(wilde_part_5)\n",
    "wilde_part_5_corpus_freq = list(nltk.FreqDist(wilde_part_5_corpus).most_common(50))\n",
    "wilde_part_5_features = features_generator(wilde_part_5_corpus_freq, wilde_part_5)\n",
    "df_wilde_5 = pd.DataFrame.from_dict(wilde_part_5_features, orient = 'index')\n",
    "wilde_zscores_5 = zscores(df_wilde_5)\n",
    "wilde_zscores_5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_test_5_features = features_generator(wilde_part_5_corpus_freq, wilde_test_5)\n",
    "df_wilde_test_5 = pd.DataFrame.from_dict(wilde_test_5_features, orient = 'index')\n",
    "wilde_zscores_test_5 = zscores(df_wilde_test_5)\n",
    "wilde_zscores_test_5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_5_deltas = get_deltas(wilde_characters, wilde_zscores_test_5, wilde_zscores_5)\n",
    "predictions_wilde_5 = wilde_5_deltas.idxmin()\n",
    "wilde_df_5 = pd.DataFrame(predictions_wilde_5, columns = ['Part_5'])\n",
    "wilde_df_5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_bigram_df_5 = model_predictions(wilde_bigrams_part_5, wilde_bigrams_test_5, 50, wilde_characters, 'Part_5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oscar Wilde Characters Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Burrows'  Delta Method Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Part_1</th>\n",
       "      <th>Part_2</th>\n",
       "      <th>Part_3</th>\n",
       "      <th>Part_4</th>\n",
       "      <th>Part_5</th>\n",
       "      <th>Results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Algernon</th>\n",
       "      <td>Jack</td>\n",
       "      <td>Chiltern</td>\n",
       "      <td>Algernon</td>\n",
       "      <td>Jack</td>\n",
       "      <td>Algernon</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Allonby</th>\n",
       "      <td>Goring</td>\n",
       "      <td>Illingorth</td>\n",
       "      <td>Chiltern</td>\n",
       "      <td>Allonby</td>\n",
       "      <td>Allonby</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Berwick</th>\n",
       "      <td>Berwick</td>\n",
       "      <td>Hunstanton</td>\n",
       "      <td>Hunstanton</td>\n",
       "      <td>Goring</td>\n",
       "      <td>Hunstanton</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bracknell</th>\n",
       "      <td>Bracknell</td>\n",
       "      <td>Goring</td>\n",
       "      <td>Jack</td>\n",
       "      <td>Bracknell</td>\n",
       "      <td>Bracknell</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Caversham</th>\n",
       "      <td>Goring</td>\n",
       "      <td>Lady Windermere</td>\n",
       "      <td>Caversham</td>\n",
       "      <td>Caversham</td>\n",
       "      <td>Darlington</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cecily</th>\n",
       "      <td>Algernon</td>\n",
       "      <td>Jack</td>\n",
       "      <td>Cecily</td>\n",
       "      <td>Cecily</td>\n",
       "      <td>Cecily</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cheveley</th>\n",
       "      <td>Cheveley</td>\n",
       "      <td>Cheveley</td>\n",
       "      <td>Cheveley</td>\n",
       "      <td>Cheveley</td>\n",
       "      <td>Goring</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chiltern</th>\n",
       "      <td>Chiltern</td>\n",
       "      <td>Chiltern</td>\n",
       "      <td>Chiltern</td>\n",
       "      <td>Jack</td>\n",
       "      <td>Chiltern</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Darlington</th>\n",
       "      <td>Cecily</td>\n",
       "      <td>Cheveley</td>\n",
       "      <td>Darlington</td>\n",
       "      <td>Goring</td>\n",
       "      <td>Cecily</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Erlynne</th>\n",
       "      <td>Lady Windermere</td>\n",
       "      <td>Erlynne</td>\n",
       "      <td>Erlynne</td>\n",
       "      <td>Lady Windermere</td>\n",
       "      <td>Goring</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gerald</th>\n",
       "      <td>Gerald</td>\n",
       "      <td>Lady Windermere</td>\n",
       "      <td>Jack</td>\n",
       "      <td>Lady Windermere</td>\n",
       "      <td>Cecily</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Goring</th>\n",
       "      <td>Illingorth</td>\n",
       "      <td>Goring</td>\n",
       "      <td>Goring</td>\n",
       "      <td>Goring</td>\n",
       "      <td>Goring</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gwendolen</th>\n",
       "      <td>Cheveley</td>\n",
       "      <td>Algernon</td>\n",
       "      <td>Gwendolen</td>\n",
       "      <td>Gwendolen</td>\n",
       "      <td>Cecily</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hunstanton</th>\n",
       "      <td>Hunstanton</td>\n",
       "      <td>Hunstanton</td>\n",
       "      <td>Hunstanton</td>\n",
       "      <td>Hunstanton</td>\n",
       "      <td>Hunstanton</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Illingorth</th>\n",
       "      <td>Illingorth</td>\n",
       "      <td>Goring</td>\n",
       "      <td>Illingorth</td>\n",
       "      <td>Goring</td>\n",
       "      <td>Goring</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jack</th>\n",
       "      <td>Jack</td>\n",
       "      <td>Goring</td>\n",
       "      <td>Jack</td>\n",
       "      <td>Jack</td>\n",
       "      <td>Cheveley</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lady Chiltern</th>\n",
       "      <td>Lady Chiltern</td>\n",
       "      <td>Lady Windermere</td>\n",
       "      <td>Lady Windermere</td>\n",
       "      <td>Lady Chiltern</td>\n",
       "      <td>Lady Chiltern</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lady Windermere</th>\n",
       "      <td>Lady Windermere</td>\n",
       "      <td>Lady Windermere</td>\n",
       "      <td>Lady Windermere</td>\n",
       "      <td>Lady Windermere</td>\n",
       "      <td>Goring</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lord Windermere</th>\n",
       "      <td>Lord Windermere</td>\n",
       "      <td>Lord Windermere</td>\n",
       "      <td>Lady Windermere</td>\n",
       "      <td>Lady Windermere</td>\n",
       "      <td>Lady Windermere</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mabel</th>\n",
       "      <td>Cheveley</td>\n",
       "      <td>Cheveley</td>\n",
       "      <td>Cheveley</td>\n",
       "      <td>Mabel</td>\n",
       "      <td>Goring</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mrs Artbuthnot</th>\n",
       "      <td>Mrs Artbuthnot</td>\n",
       "      <td>Cheveley</td>\n",
       "      <td>Lady Windermere</td>\n",
       "      <td>Lady Windermere</td>\n",
       "      <td>Mrs Artbuthnot</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Part_1           Part_2           Part_3  \\\n",
       "Algernon                    Jack         Chiltern         Algernon   \n",
       "Allonby                   Goring       Illingorth         Chiltern   \n",
       "Berwick                  Berwick       Hunstanton       Hunstanton   \n",
       "Bracknell              Bracknell           Goring             Jack   \n",
       "Caversham                 Goring  Lady Windermere        Caversham   \n",
       "Cecily                  Algernon             Jack           Cecily   \n",
       "Cheveley                Cheveley         Cheveley         Cheveley   \n",
       "Chiltern                Chiltern         Chiltern         Chiltern   \n",
       "Darlington                Cecily         Cheveley       Darlington   \n",
       "Erlynne          Lady Windermere          Erlynne          Erlynne   \n",
       "Gerald                    Gerald  Lady Windermere             Jack   \n",
       "Goring                Illingorth           Goring           Goring   \n",
       "Gwendolen               Cheveley         Algernon        Gwendolen   \n",
       "Hunstanton            Hunstanton       Hunstanton       Hunstanton   \n",
       "Illingorth            Illingorth           Goring       Illingorth   \n",
       "Jack                        Jack           Goring             Jack   \n",
       "Lady Chiltern      Lady Chiltern  Lady Windermere  Lady Windermere   \n",
       "Lady Windermere  Lady Windermere  Lady Windermere  Lady Windermere   \n",
       "Lord Windermere  Lord Windermere  Lord Windermere  Lady Windermere   \n",
       "Mabel                   Cheveley         Cheveley         Cheveley   \n",
       "Mrs Artbuthnot    Mrs Artbuthnot         Cheveley  Lady Windermere   \n",
       "\n",
       "                          Part_4           Part_5  Results  \n",
       "Algernon                    Jack         Algernon      0.4  \n",
       "Allonby                  Allonby          Allonby      0.4  \n",
       "Berwick                   Goring       Hunstanton      0.2  \n",
       "Bracknell              Bracknell        Bracknell      0.6  \n",
       "Caversham              Caversham       Darlington      0.4  \n",
       "Cecily                    Cecily           Cecily      0.6  \n",
       "Cheveley                Cheveley           Goring      0.8  \n",
       "Chiltern                    Jack         Chiltern      0.8  \n",
       "Darlington                Goring           Cecily      0.2  \n",
       "Erlynne          Lady Windermere           Goring      0.4  \n",
       "Gerald           Lady Windermere           Cecily      0.2  \n",
       "Goring                    Goring           Goring      0.8  \n",
       "Gwendolen              Gwendolen           Cecily      0.4  \n",
       "Hunstanton            Hunstanton       Hunstanton      1.0  \n",
       "Illingorth                Goring           Goring      0.4  \n",
       "Jack                        Jack         Cheveley      0.6  \n",
       "Lady Chiltern      Lady Chiltern    Lady Chiltern      0.6  \n",
       "Lady Windermere  Lady Windermere           Goring      0.8  \n",
       "Lord Windermere  Lady Windermere  Lady Windermere      0.4  \n",
       "Mabel                      Mabel           Goring      0.2  \n",
       "Mrs Artbuthnot   Lady Windermere   Mrs Artbuthnot      0.4  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We concatenate all the partition predictions data frames into a single one\n",
    "# and then we create the result column as an accuracy metric\n",
    "wilde_results = pd.concat([wilde_df_1, wilde_df_2, wilde_df_3, wilde_df_4, wilde_df_5], axis=1)\n",
    "wilde_results = success_rate(wilde_results)\n",
    "wilde_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5047619047619049"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wilde_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results of the top 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Part_1</th>\n",
       "      <th>Part_2</th>\n",
       "      <th>Part_3</th>\n",
       "      <th>Part_4</th>\n",
       "      <th>Part_5</th>\n",
       "      <th>Results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Goring</th>\n",
       "      <td>Illingorth</td>\n",
       "      <td>Goring</td>\n",
       "      <td>Goring</td>\n",
       "      <td>Goring</td>\n",
       "      <td>Goring</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chiltern</th>\n",
       "      <td>Chiltern</td>\n",
       "      <td>Chiltern</td>\n",
       "      <td>Chiltern</td>\n",
       "      <td>Jack</td>\n",
       "      <td>Chiltern</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cheveley</th>\n",
       "      <td>Cheveley</td>\n",
       "      <td>Cheveley</td>\n",
       "      <td>Cheveley</td>\n",
       "      <td>Cheveley</td>\n",
       "      <td>Goring</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Illingorth</th>\n",
       "      <td>Illingorth</td>\n",
       "      <td>Goring</td>\n",
       "      <td>Illingorth</td>\n",
       "      <td>Goring</td>\n",
       "      <td>Goring</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lady Windermere</th>\n",
       "      <td>Lady Windermere</td>\n",
       "      <td>Lady Windermere</td>\n",
       "      <td>Lady Windermere</td>\n",
       "      <td>Lady Windermere</td>\n",
       "      <td>Goring</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algernon</th>\n",
       "      <td>Jack</td>\n",
       "      <td>Chiltern</td>\n",
       "      <td>Algernon</td>\n",
       "      <td>Jack</td>\n",
       "      <td>Algernon</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jack</th>\n",
       "      <td>Jack</td>\n",
       "      <td>Goring</td>\n",
       "      <td>Jack</td>\n",
       "      <td>Jack</td>\n",
       "      <td>Cheveley</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Part_1           Part_2           Part_3  \\\n",
       "Goring                Illingorth           Goring           Goring   \n",
       "Chiltern                Chiltern         Chiltern         Chiltern   \n",
       "Cheveley                Cheveley         Cheveley         Cheveley   \n",
       "Illingorth            Illingorth           Goring       Illingorth   \n",
       "Lady Windermere  Lady Windermere  Lady Windermere  Lady Windermere   \n",
       "Algernon                    Jack         Chiltern         Algernon   \n",
       "Jack                        Jack           Goring             Jack   \n",
       "\n",
       "                          Part_4    Part_5  Results  \n",
       "Goring                    Goring    Goring      0.8  \n",
       "Chiltern                    Jack  Chiltern      0.8  \n",
       "Cheveley                Cheveley    Goring      0.8  \n",
       "Illingorth                Goring    Goring      0.4  \n",
       "Lady Windermere  Lady Windermere    Goring      0.8  \n",
       "Algernon                    Jack  Algernon      0.4  \n",
       "Jack                        Jack  Cheveley      0.6  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We get the result of the top 7 characters (+4000 words), with the purpose of exploring\n",
    "# their accuracy scores, when they are predicted among the whole list of characters, \n",
    "# where 4000 words ones could be treted as noise\n",
    "wilde_top_7_results = wilde_results.filter(items = wilde_top_7_characters, axis=0)\n",
    "wilde_top_7_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6571428571428573"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wilde_top_7_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Burrows' Delta Method with word bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4095238095238095"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wilde_bigram_results = pd.concat([wilde_bigram_df_1, wilde_bigram_df_2, wilde_bigram_df_3, wilde_bigram_df_4, wilde_bigram_df_5], axis =1)\n",
    "wilde_bigram_results = success_rate(wilde_bigram_results)\n",
    "wilde_bigram_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# George Bernard Shaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characters (+1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_characters = {**pygmalion_characters, **androcles_and_the_lion_characters, **caesar_and_cleopatra_characters, **candida_characters, **man_and_superman_characters}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shaw_characters.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_characters_split = corpus_split(shaw_characters, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_characters_partition = split_partitions(shaw_characters_split)\n",
    "shaw_characters_test = split_test(shaw_characters_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_part_1 = dict_word_tokenizer(shaw_characters_partition[0])\n",
    "shaw_part_2 = dict_word_tokenizer(shaw_characters_partition[1])\n",
    "shaw_part_3 = dict_word_tokenizer(shaw_characters_partition[2])\n",
    "shaw_part_4 = dict_word_tokenizer(shaw_characters_partition[3])\n",
    "shaw_part_5 = dict_word_tokenizer(shaw_characters_partition[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_test_1 = dict_word_tokenizer(shaw_characters_test[0])\n",
    "shaw_test_2 = dict_word_tokenizer(shaw_characters_test[1])\n",
    "shaw_test_3 = dict_word_tokenizer(shaw_characters_test[2])\n",
    "shaw_test_4 = dict_word_tokenizer(shaw_characters_test[3])\n",
    "shaw_test_5 = dict_word_tokenizer(shaw_characters_test[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_bigrams_part_1 = ngrams_tokenizer(dict_word_tokenizer(shaw_characters_partition[0]), 2)\n",
    "shaw_bigrams_part_2 = ngrams_tokenizer(dict_word_tokenizer(shaw_characters_partition[1]), 2)\n",
    "shaw_bigrams_part_3 = ngrams_tokenizer(dict_word_tokenizer(shaw_characters_partition[2]), 2)\n",
    "shaw_bigrams_part_4 = ngrams_tokenizer(dict_word_tokenizer(shaw_characters_partition[3]), 2)\n",
    "shaw_bigrams_part_5 = ngrams_tokenizer(dict_word_tokenizer(shaw_characters_partition[4]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_bigrams_test_1 = ngrams_tokenizer(dict_word_tokenizer(shaw_characters_test[0]), 2)\n",
    "shaw_bigrams_test_2 = ngrams_tokenizer(dict_word_tokenizer(shaw_characters_test[1]), 2)\n",
    "shaw_bigrams_test_3 = ngrams_tokenizer(dict_word_tokenizer(shaw_characters_test[2]), 2)\n",
    "shaw_bigrams_test_4 = ngrams_tokenizer(dict_word_tokenizer(shaw_characters_test[3]), 2)\n",
    "shaw_bigrams_test_5 = ngrams_tokenizer(dict_word_tokenizer(shaw_characters_test[4]), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 7 characters (+4000 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_top_7_characters = ['Higgins', 'Liza', 'Caesar', 'Cleopatra', 'Morell', 'Tanner', 'Don Juan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_top_7 = {}\n",
    "for key in shaw_characters:\n",
    "    if key in shaw_top_7_characters:\n",
    "        shaw_top_7[key] = shaw_characters[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_part_1_corpus = whole_corpus_generator(shaw_part_1)\n",
    "shaw_part_1_corpus_freq = list(nltk.FreqDist(shaw_part_1_corpus).most_common(50))\n",
    "shaw_part_1_features = features_generator(shaw_part_1_corpus_freq, shaw_part_1)\n",
    "df_shaw_1 = pd.DataFrame.from_dict(shaw_part_1_features, orient = 'index')\n",
    "shaw_zscores_1 = zscores(df_shaw_1)\n",
    "shaw_zscores_1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_test_1_features = features_generator(shaw_part_1_corpus_freq, shaw_test_1)\n",
    "df_shaw_test_1 = pd.DataFrame.from_dict(shaw_test_1_features, orient = 'index')\n",
    "shaw_zscores_test_1 = zscores(df_shaw_test_1)\n",
    "shaw_zscores_test_1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_1_deltas = get_deltas(shaw_characters, shaw_zscores_test_1, shaw_zscores_1)\n",
    "predictions_shaw_1 = shaw_1_deltas.idxmin()\n",
    "shaw_df_1 = pd.DataFrame(predictions_shaw_1, columns = ['Part_1'])\n",
    "shaw_df_1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_bigram_df_1 = model_predictions(shaw_bigrams_part_1, shaw_bigrams_test_1, 50, shaw_characters, 'Part_1')\n",
    "shaw_bigram_df_1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_part_2_corpus = whole_corpus_generator(shaw_part_2)\n",
    "shaw_part_2_corpus_freq = list(nltk.FreqDist(shaw_part_2_corpus).most_common(50))\n",
    "shaw_part_2_features = features_generator(shaw_part_2_corpus_freq, shaw_part_2)\n",
    "df_shaw_2 = pd.DataFrame.from_dict(shaw_part_2_features, orient = 'index')\n",
    "shaw_zscores_2 = zscores(df_shaw_2)\n",
    "shaw_zscores_2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_test_2_features = features_generator(shaw_part_2_corpus_freq, shaw_test_2)\n",
    "df_shaw_test_2 = pd.DataFrame.from_dict(shaw_test_2_features, orient = 'index')\n",
    "shaw_zscores_test_2 = zscores(df_shaw_test_2)\n",
    "shaw_zscores_test_2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_2_deltas = get_deltas(shaw_characters, shaw_zscores_test_2, shaw_zscores_2)\n",
    "predictions_shaw_2 = shaw_2_deltas.idxmin()\n",
    "shaw_df_2 = pd.DataFrame(predictions_shaw_2, columns = ['Part_2'])\n",
    "shaw_df_2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_bigram_df_2 = model_predictions(shaw_bigrams_part_2, shaw_bigrams_test_2, 50, shaw_characters, 'Part_2')\n",
    "shaw_bigram_df_2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_part_3_corpus = whole_corpus_generator(shaw_part_3)\n",
    "shaw_part_3_corpus_freq = list(nltk.FreqDist(shaw_part_3_corpus).most_common(50))\n",
    "shaw_part_3_features = features_generator(shaw_part_3_corpus_freq, shaw_part_3)\n",
    "df_shaw_3 = pd.DataFrame.from_dict(shaw_part_3_features, orient = 'index')\n",
    "shaw_zscores_3 = zscores(df_shaw_3)\n",
    "shaw_zscores_3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_test_3_features = features_generator(shaw_part_3_corpus_freq, shaw_test_3)\n",
    "df_shaw_test_3 = pd.DataFrame.from_dict(shaw_test_3_features, orient = 'index')\n",
    "shaw_zscores_test_3 = zscores(df_shaw_test_3)\n",
    "shaw_zscores_test_3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_3_deltas = get_deltas(shaw_characters, shaw_zscores_test_3, shaw_zscores_3)\n",
    "predictions_shaw_3 = shaw_3_deltas.idxmin()\n",
    "shaw_df_3 = pd.DataFrame(predictions_shaw_3, columns = ['Part_3'])\n",
    "shaw_df_3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_bigram_df_3 = model_predictions(shaw_bigrams_part_3, shaw_bigrams_test_3, 50, shaw_characters, 'Part_3')\n",
    "shaw_bigram_df_3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_part_4_corpus = whole_corpus_generator(shaw_part_4)\n",
    "shaw_part_4_corpus_freq = list(nltk.FreqDist(shaw_part_4_corpus).most_common(50))\n",
    "shaw_part_4_features = features_generator(shaw_part_4_corpus_freq, shaw_part_4)\n",
    "df_shaw_4 = pd.DataFrame.from_dict(shaw_part_4_features, orient = 'index')\n",
    "shaw_zscores_4 = zscores(df_shaw_4)\n",
    "shaw_zscores_4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_test_4_features = features_generator(shaw_part_4_corpus_freq, shaw_test_4)\n",
    "df_shaw_test_4 = pd.DataFrame.from_dict(shaw_test_4_features, orient = 'index')\n",
    "shaw_zscores_test_4 = zscores(df_shaw_test_4)\n",
    "shaw_zscores_test_4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_4_deltas = get_deltas(shaw_characters, shaw_zscores_test_4, shaw_zscores_4)\n",
    "predictions_shaw_4 = shaw_4_deltas.idxmin()\n",
    "shaw_df_4 = pd.DataFrame(predictions_shaw_4, columns = ['Part_4'])\n",
    "shaw_df_4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_bigram_df_4 = model_predictions(shaw_bigrams_part_4, shaw_bigrams_test_4, 50, shaw_characters, 'Part_4')\n",
    "shaw_bigram_df_4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_part_5_corpus = whole_corpus_generator(shaw_part_5)\n",
    "shaw_part_5_corpus_freq = list(nltk.FreqDist(shaw_part_5_corpus).most_common(50))\n",
    "shaw_part_5_features = features_generator(shaw_part_5_corpus_freq, shaw_part_5)\n",
    "df_shaw_5 = pd.DataFrame.from_dict(shaw_part_5_features, orient = 'index')\n",
    "shaw_zscores_5 = zscores(df_shaw_5)\n",
    "shaw_zscores_5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_test_5_features = features_generator(shaw_part_5_corpus_freq, shaw_test_5)\n",
    "df_shaw_test_5 = pd.DataFrame.from_dict(shaw_test_5_features, orient = 'index')\n",
    "shaw_zscores_test_5 = zscores(df_shaw_test_5)\n",
    "shaw_zscores_test_5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_5_deltas = get_deltas(shaw_characters, shaw_zscores_test_5, shaw_zscores_5)\n",
    "predictions_shaw_5 = shaw_5_deltas.idxmin()\n",
    "shaw_df_5 = pd.DataFrame(predictions_shaw_5, columns = ['Part_5'])\n",
    "shaw_df_5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaw_bigram_df_5 = model_predictions(shaw_bigrams_part_5, shaw_bigrams_test_5, 50, shaw_characters, 'Part_5')\n",
    "shaw_bigram_df_5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## George Bernard Shaw Characters Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Part_1</th>\n",
       "      <th>Part_2</th>\n",
       "      <th>Part_3</th>\n",
       "      <th>Part_4</th>\n",
       "      <th>Part_5</th>\n",
       "      <th>Results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Androcles</th>\n",
       "      <td>Tanner</td>\n",
       "      <td>Androcles</td>\n",
       "      <td>Lavinia</td>\n",
       "      <td>Morell</td>\n",
       "      <td>Androcles</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ann</th>\n",
       "      <td>Ann</td>\n",
       "      <td>Ann</td>\n",
       "      <td>Ann</td>\n",
       "      <td>Ann</td>\n",
       "      <td>Ann</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apollodorus</th>\n",
       "      <td>Don Juan</td>\n",
       "      <td>Mendoza</td>\n",
       "      <td>Apollodorus</td>\n",
       "      <td>Mendoza</td>\n",
       "      <td>Apollodorus</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Burgess</th>\n",
       "      <td>Morell</td>\n",
       "      <td>Burgess</td>\n",
       "      <td>Burgess</td>\n",
       "      <td>Burgess</td>\n",
       "      <td>Burgess</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Caesar</th>\n",
       "      <td>Caesar</td>\n",
       "      <td>Caesar</td>\n",
       "      <td>Caesar</td>\n",
       "      <td>Caesar</td>\n",
       "      <td>Caesar</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Candida</th>\n",
       "      <td>Morell</td>\n",
       "      <td>Candida</td>\n",
       "      <td>Candida</td>\n",
       "      <td>Candida</td>\n",
       "      <td>Candida</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Captain</th>\n",
       "      <td>Captain</td>\n",
       "      <td>Caesar</td>\n",
       "      <td>Captain</td>\n",
       "      <td>Caesar</td>\n",
       "      <td>Captain</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cleopatra</th>\n",
       "      <td>Cleopatra</td>\n",
       "      <td>Cleopatra</td>\n",
       "      <td>Cleopatra</td>\n",
       "      <td>Cleopatra</td>\n",
       "      <td>Cleopatra</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Don Juan</th>\n",
       "      <td>Don Juan</td>\n",
       "      <td>Don Juan</td>\n",
       "      <td>Don Juan</td>\n",
       "      <td>Don Juan</td>\n",
       "      <td>Don Juan</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doolittle</th>\n",
       "      <td>Doolittle</td>\n",
       "      <td>Doolittle</td>\n",
       "      <td>Doolittle</td>\n",
       "      <td>Doolittle</td>\n",
       "      <td>Tanner</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Higgins</th>\n",
       "      <td>Higgins</td>\n",
       "      <td>Higgins</td>\n",
       "      <td>Higgins</td>\n",
       "      <td>Higgins</td>\n",
       "      <td>Higgins</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lavinia</th>\n",
       "      <td>Morell</td>\n",
       "      <td>Morell</td>\n",
       "      <td>Caesar</td>\n",
       "      <td>Lavinia</td>\n",
       "      <td>Morell</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Liza</th>\n",
       "      <td>Liza</td>\n",
       "      <td>Liza</td>\n",
       "      <td>Liza</td>\n",
       "      <td>Liza</td>\n",
       "      <td>Liza</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marchbanks</th>\n",
       "      <td>Marchbanks</td>\n",
       "      <td>Marchbanks</td>\n",
       "      <td>Marchbanks</td>\n",
       "      <td>Morell</td>\n",
       "      <td>Marchbanks</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mendoza</th>\n",
       "      <td>Mendoza</td>\n",
       "      <td>Tanner</td>\n",
       "      <td>Tanner</td>\n",
       "      <td>Tanner</td>\n",
       "      <td>Don Juan</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Morell</th>\n",
       "      <td>Tanner</td>\n",
       "      <td>Morell</td>\n",
       "      <td>Morell</td>\n",
       "      <td>Morell</td>\n",
       "      <td>Morell</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Octavius</th>\n",
       "      <td>Marchbanks</td>\n",
       "      <td>Octavius</td>\n",
       "      <td>Ramsden</td>\n",
       "      <td>Morell</td>\n",
       "      <td>Octavius</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pickering</th>\n",
       "      <td>Pickering</td>\n",
       "      <td>Pickering</td>\n",
       "      <td>Higgins</td>\n",
       "      <td>Mendoza</td>\n",
       "      <td>Pickering</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ramsden</th>\n",
       "      <td>Morell</td>\n",
       "      <td>Tanner</td>\n",
       "      <td>Ramsden</td>\n",
       "      <td>Ramsden</td>\n",
       "      <td>Tanner</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rufio</th>\n",
       "      <td>Tanner</td>\n",
       "      <td>Rufio</td>\n",
       "      <td>Tanner</td>\n",
       "      <td>Rufio</td>\n",
       "      <td>Rufio</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tanner</th>\n",
       "      <td>Tanner</td>\n",
       "      <td>Tanner</td>\n",
       "      <td>Tanner</td>\n",
       "      <td>Tanner</td>\n",
       "      <td>Tanner</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Devil</th>\n",
       "      <td>The Devil</td>\n",
       "      <td>The Devil</td>\n",
       "      <td>Don Juan</td>\n",
       "      <td>Don Juan</td>\n",
       "      <td>The Devil</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Violet</th>\n",
       "      <td>Ann</td>\n",
       "      <td>Morell</td>\n",
       "      <td>Candida</td>\n",
       "      <td>Morell</td>\n",
       "      <td>Morell</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Part_1      Part_2       Part_3     Part_4       Part_5  \\\n",
       "Androcles        Tanner   Androcles      Lavinia     Morell    Androcles   \n",
       "Ann                 Ann         Ann          Ann        Ann          Ann   \n",
       "Apollodorus    Don Juan     Mendoza  Apollodorus    Mendoza  Apollodorus   \n",
       "Burgess          Morell     Burgess      Burgess    Burgess      Burgess   \n",
       "Caesar           Caesar      Caesar       Caesar     Caesar       Caesar   \n",
       "Candida          Morell     Candida      Candida    Candida      Candida   \n",
       "Captain         Captain      Caesar      Captain     Caesar      Captain   \n",
       "Cleopatra     Cleopatra   Cleopatra    Cleopatra  Cleopatra    Cleopatra   \n",
       "Don Juan       Don Juan    Don Juan     Don Juan   Don Juan     Don Juan   \n",
       "Doolittle     Doolittle   Doolittle    Doolittle  Doolittle       Tanner   \n",
       "Higgins         Higgins     Higgins      Higgins    Higgins      Higgins   \n",
       "Lavinia          Morell      Morell       Caesar    Lavinia       Morell   \n",
       "Liza               Liza        Liza         Liza       Liza         Liza   \n",
       "Marchbanks   Marchbanks  Marchbanks   Marchbanks     Morell   Marchbanks   \n",
       "Mendoza         Mendoza      Tanner       Tanner     Tanner     Don Juan   \n",
       "Morell           Tanner      Morell       Morell     Morell       Morell   \n",
       "Octavius     Marchbanks    Octavius      Ramsden     Morell     Octavius   \n",
       "Pickering     Pickering   Pickering      Higgins    Mendoza    Pickering   \n",
       "Ramsden          Morell      Tanner      Ramsden    Ramsden       Tanner   \n",
       "Rufio            Tanner       Rufio       Tanner      Rufio        Rufio   \n",
       "Tanner           Tanner      Tanner       Tanner     Tanner       Tanner   \n",
       "The Devil     The Devil   The Devil     Don Juan   Don Juan    The Devil   \n",
       "Violet              Ann      Morell      Candida     Morell       Morell   \n",
       "\n",
       "             Results  \n",
       "Androcles        0.4  \n",
       "Ann              1.0  \n",
       "Apollodorus      0.4  \n",
       "Burgess          0.8  \n",
       "Caesar           1.0  \n",
       "Candida          0.8  \n",
       "Captain          0.6  \n",
       "Cleopatra        1.0  \n",
       "Don Juan         1.0  \n",
       "Doolittle        0.8  \n",
       "Higgins          1.0  \n",
       "Lavinia          0.2  \n",
       "Liza             1.0  \n",
       "Marchbanks       0.8  \n",
       "Mendoza          0.2  \n",
       "Morell           0.8  \n",
       "Octavius         0.4  \n",
       "Pickering        0.6  \n",
       "Ramsden          0.4  \n",
       "Rufio            0.6  \n",
       "Tanner           1.0  \n",
       "The Devil        0.6  \n",
       "Violet           0.0  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shaw_results = pd.concat([shaw_df_1, shaw_df_2, shaw_df_3, shaw_df_4, shaw_df_5], axis=1)\n",
    "shaw_results = success_rate(shaw_results)\n",
    "shaw_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6695652173913043"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shaw_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Part_1</th>\n",
       "      <th>Part_2</th>\n",
       "      <th>Part_3</th>\n",
       "      <th>Part_4</th>\n",
       "      <th>Part_5</th>\n",
       "      <th>Results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Higgins</th>\n",
       "      <td>Higgins</td>\n",
       "      <td>Higgins</td>\n",
       "      <td>Higgins</td>\n",
       "      <td>Higgins</td>\n",
       "      <td>Higgins</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Liza</th>\n",
       "      <td>Liza</td>\n",
       "      <td>Liza</td>\n",
       "      <td>Liza</td>\n",
       "      <td>Liza</td>\n",
       "      <td>Liza</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Caesar</th>\n",
       "      <td>Caesar</td>\n",
       "      <td>Caesar</td>\n",
       "      <td>Caesar</td>\n",
       "      <td>Caesar</td>\n",
       "      <td>Caesar</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cleopatra</th>\n",
       "      <td>Cleopatra</td>\n",
       "      <td>Cleopatra</td>\n",
       "      <td>Cleopatra</td>\n",
       "      <td>Cleopatra</td>\n",
       "      <td>Cleopatra</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Morell</th>\n",
       "      <td>Tanner</td>\n",
       "      <td>Morell</td>\n",
       "      <td>Morell</td>\n",
       "      <td>Morell</td>\n",
       "      <td>Morell</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tanner</th>\n",
       "      <td>Tanner</td>\n",
       "      <td>Tanner</td>\n",
       "      <td>Tanner</td>\n",
       "      <td>Tanner</td>\n",
       "      <td>Tanner</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Don Juan</th>\n",
       "      <td>Don Juan</td>\n",
       "      <td>Don Juan</td>\n",
       "      <td>Don Juan</td>\n",
       "      <td>Don Juan</td>\n",
       "      <td>Don Juan</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Part_1     Part_2     Part_3     Part_4     Part_5  Results\n",
       "Higgins      Higgins    Higgins    Higgins    Higgins    Higgins      1.0\n",
       "Liza            Liza       Liza       Liza       Liza       Liza      1.0\n",
       "Caesar        Caesar     Caesar     Caesar     Caesar     Caesar      1.0\n",
       "Cleopatra  Cleopatra  Cleopatra  Cleopatra  Cleopatra  Cleopatra      1.0\n",
       "Morell        Tanner     Morell     Morell     Morell     Morell      0.8\n",
       "Tanner        Tanner     Tanner     Tanner     Tanner     Tanner      1.0\n",
       "Don Juan    Don Juan   Don Juan   Don Juan   Don Juan   Don Juan      1.0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shaw_top_7_results = shaw_results.filter(items = shaw_top_7_characters, axis=0)\n",
    "shaw_top_7_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9714285714285714"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shaw_top_7_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32173913043478264"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shaw_bigram_results = pd.concat([shaw_bigram_df_1, shaw_bigram_df_2, shaw_bigram_df_3, shaw_bigram_df_4, shaw_bigram_df_5], axis =1)\n",
    "shaw_bigram_results = success_rate(shaw_bigram_results)\n",
    "shaw_bigram_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ben Jonson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characters (+1500 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_characters = {**cynthias_revels_characters, **every_man_on_his_humour_characters, **volpone_or_the_fox_characters, **the_alchemist_characters}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jonson_characters.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_characters_split = corpus_split(jonson_characters, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_characters_partition = split_partitions(jonson_characters_split)\n",
    "jonson_characters_test = split_test(jonson_characters_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_part_1 = dict_word_tokenizer(jonson_characters_partition[0])\n",
    "jonson_part_2 = dict_word_tokenizer(jonson_characters_partition[1])\n",
    "jonson_part_3 = dict_word_tokenizer(jonson_characters_partition[2])\n",
    "jonson_part_4 = dict_word_tokenizer(jonson_characters_partition[3])\n",
    "jonson_part_5 = dict_word_tokenizer(jonson_characters_partition[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_test_1 = dict_word_tokenizer(jonson_characters_test[0])\n",
    "jonson_test_2 = dict_word_tokenizer(jonson_characters_test[1])\n",
    "jonson_test_3 = dict_word_tokenizer(jonson_characters_test[2])\n",
    "jonson_test_4 = dict_word_tokenizer(jonson_characters_test[3])\n",
    "jonson_test_5 = dict_word_tokenizer(jonson_characters_test[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_bigrams_part_1 = ngrams_tokenizer(dict_word_tokenizer(jonson_characters_partition[0]), 2)\n",
    "jonson_bigrams_part_2 = ngrams_tokenizer(dict_word_tokenizer(jonson_characters_partition[1]), 2)\n",
    "jonson_bigrams_part_3 = ngrams_tokenizer(dict_word_tokenizer(jonson_characters_partition[2]), 2)\n",
    "jonson_bigrams_part_4 = ngrams_tokenizer(dict_word_tokenizer(jonson_characters_partition[3]), 2)\n",
    "jonson_bigrams_part_5 = ngrams_tokenizer(dict_word_tokenizer(jonson_characters_partition[4]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_bigrams_test_1 = ngrams_tokenizer(dict_word_tokenizer(jonson_characters_test[0]), 2)\n",
    "jonson_bigrams_test_2 = ngrams_tokenizer(dict_word_tokenizer(jonson_characters_test[1]), 2)\n",
    "jonson_bigrams_test_3 = ngrams_tokenizer(dict_word_tokenizer(jonson_characters_test[2]), 2)\n",
    "jonson_bigrams_test_4 = ngrams_tokenizer(dict_word_tokenizer(jonson_characters_test[3]), 2)\n",
    "jonson_bigrams_test_5 = ngrams_tokenizer(dict_word_tokenizer(jonson_characters_test[4]), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Top 7 characters (+4000 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_top_7_characters = ['Mercury', 'Amorphus', 'Crites', 'Volpone', 'Mosca', 'Face', 'Subtle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_top_7 = {}\n",
    "for key in jonson_characters:\n",
    "    if key in jonson_top_7_characters:\n",
    "        jonson_top_7[key] = jonson_characters[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train z - sccores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_part_1_corpus = whole_corpus_generator(jonson_part_1)\n",
    "jonson_part_1_corpus_freq = list(nltk.FreqDist(jonson_part_1_corpus).most_common(50))\n",
    "jonson_part_1_features = features_generator(jonson_part_1_corpus_freq, jonson_part_1)\n",
    "df_jonson_1 = pd.DataFrame.from_dict(jonson_part_1_features, orient = 'index')\n",
    "jonson_zscores_1 = zscores(df_jonson_1)\n",
    "jonson_zscores_1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_test_1_features = features_generator(jonson_part_1_corpus_freq, jonson_test_1)\n",
    "df_jonson_test_1 = pd.DataFrame.from_dict(jonson_test_1_features, orient = 'index')\n",
    "jonson_zscores_test_1 = zscores(df_jonson_test_1)\n",
    "jonson_zscores_test_1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_1_deltas = get_deltas(jonson_characters, jonson_zscores_test_1, jonson_zscores_1)\n",
    "predictions_jonson_1 = jonson_1_deltas.idxmin()\n",
    "jonson_df_1 = pd.DataFrame(predictions_jonson_1, columns = ['Part_1'])\n",
    "jonson_df_1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_bigram_df_1 = model_predictions(jonson_bigrams_part_1, jonson_bigrams_test_1, 50, jonson_characters, 'Part_1')\n",
    "jonson_bigram_df_1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_part_2_corpus = whole_corpus_generator(jonson_part_2)\n",
    "jonson_part_2_corpus_freq = list(nltk.FreqDist(jonson_part_2_corpus).most_common(50))\n",
    "jonson_part_2_features = features_generator(jonson_part_2_corpus_freq, jonson_part_2)\n",
    "df_jonson_2 = pd.DataFrame.from_dict(jonson_part_2_features, orient = 'index')\n",
    "jonson_zscores_2 = zscores(df_jonson_2)\n",
    "jonson_zscores_2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_test_2_features = features_generator(jonson_part_2_corpus_freq, jonson_test_2)\n",
    "df_jonson_test_2 = pd.DataFrame.from_dict(jonson_test_2_features, orient = 'index')\n",
    "jonson_zscores_test_2 = zscores(df_jonson_test_2)\n",
    "jonson_zscores_test_2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_2_deltas = get_deltas(jonson_characters, jonson_zscores_test_2, jonson_zscores_2)\n",
    "predictions_jonson_2 = jonson_2_deltas.idxmin()\n",
    "jonson_df_2 = pd.DataFrame(predictions_jonson_2, columns = ['Part_2'])\n",
    "jonson_df_2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_bigram_df_2 = model_predictions(jonson_bigrams_part_2, jonson_bigrams_test_2, 50, jonson_characters, 'Part_2')\n",
    "jonson_bigram_df_2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_part_3_corpus = whole_corpus_generator(jonson_part_3)\n",
    "jonson_part_3_corpus_freq = list(nltk.FreqDist(jonson_part_3_corpus).most_common(50))\n",
    "jonson_part_3_features = features_generator(jonson_part_3_corpus_freq, jonson_part_3)\n",
    "df_jonson_3 = pd.DataFrame.from_dict(jonson_part_3_features, orient = 'index')\n",
    "jonson_zscores_3 = zscores(df_jonson_3)\n",
    "jonson_zscores_3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_test_3_features = features_generator(jonson_part_3_corpus_freq, jonson_test_3)\n",
    "df_jonson_test_3 = pd.DataFrame.from_dict(jonson_test_3_features, orient = 'index')\n",
    "jonson_zscores_test_3 = zscores(df_jonson_test_3)\n",
    "jonson_zscores_test_3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_3_deltas = get_deltas(jonson_characters, jonson_zscores_test_3, jonson_zscores_3)\n",
    "predictions_jonson_3 = jonson_3_deltas.idxmin()\n",
    "jonson_df_3 = pd.DataFrame(predictions_jonson_3, columns = ['Part_3'])\n",
    "jonson_df_3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_bigram_df_3 = model_predictions(jonson_bigrams_part_3, jonson_bigrams_test_3, 50, jonson_characters, 'Part_3')\n",
    "jonson_bigram_df_3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_part_4_corpus = whole_corpus_generator(jonson_part_4)\n",
    "jonson_part_4_corpus_freq = list(nltk.FreqDist(jonson_part_4_corpus).most_common(50))\n",
    "jonson_part_4_features = features_generator(jonson_part_4_corpus_freq, jonson_part_4)\n",
    "df_jonson_4 = pd.DataFrame.from_dict(jonson_part_4_features, orient = 'index')\n",
    "jonson_zscores_4 = zscores(df_jonson_4)\n",
    "jonson_zscores_4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_test_4_features = features_generator(jonson_part_4_corpus_freq, jonson_test_4)\n",
    "df_jonson_test_4 = pd.DataFrame.from_dict(jonson_test_4_features, orient = 'index')\n",
    "jonson_zscores_test_4 = zscores(df_jonson_test_4)\n",
    "jonson_zscores_test_4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_4_deltas = get_deltas(jonson_characters, jonson_zscores_test_4, jonson_zscores_4)\n",
    "predictions_jonson_4 = jonson_4_deltas.idxmin()\n",
    "jonson_df_4 = pd.DataFrame(predictions_jonson_4, columns = ['Part_4'])\n",
    "jonson_df_4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_bigram_df_4 = model_predictions(jonson_bigrams_part_4, jonson_bigrams_test_4, 50, jonson_characters, 'Part_4')\n",
    "jonson_bigram_df_4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_part_5_corpus = whole_corpus_generator(jonson_part_5)\n",
    "jonson_part_5_corpus_freq = list(nltk.FreqDist(jonson_part_5_corpus).most_common(50))\n",
    "jonson_part_5_features = features_generator(jonson_part_5_corpus_freq, jonson_part_5)\n",
    "df_jonson_5 = pd.DataFrame.from_dict(jonson_part_5_features, orient = 'index')\n",
    "jonson_zscores_5 = zscores(df_jonson_5)\n",
    "jonson_zscores_5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_test_5_features = features_generator(jonson_part_5_corpus_freq, jonson_test_5)\n",
    "df_jonson_test_5 = pd.DataFrame.from_dict(jonson_test_5_features, orient = 'index')\n",
    "jonson_zscores_test_5 = zscores(df_jonson_test_5)\n",
    "jonson_zscores_test_5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_5_deltas = get_deltas(jonson_characters, jonson_zscores_test_5, jonson_zscores_5)\n",
    "predictions_jonson_5 = jonson_5_deltas.idxmin()\n",
    "jonson_df_5 = pd.DataFrame(predictions_jonson_5, columns = ['Part_5'])\n",
    "jonson_df_5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_bigram_df_5 = model_predictions(jonson_bigrams_part_5, jonson_bigrams_test_5, 50, jonson_characters, 'Part_5')\n",
    "jonson_bigram_df_5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ben Jonson Characters Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "jonson_results = pd.concat([jonson_df_1, jonson_df_2, jonson_df_3, jonson_df_4, jonson_df_5], axis=1)\n",
    "jonson_results = success_rate(jonson_results)\n",
    "jonson_results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49090909090909096"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jonson_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Part_1</th>\n",
       "      <th>Part_2</th>\n",
       "      <th>Part_3</th>\n",
       "      <th>Part_4</th>\n",
       "      <th>Part_5</th>\n",
       "      <th>Results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mercury</th>\n",
       "      <td>Mercury</td>\n",
       "      <td>Mercury</td>\n",
       "      <td>Mercury</td>\n",
       "      <td>Mercury</td>\n",
       "      <td>Mercury</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amorphus</th>\n",
       "      <td>Amorphus</td>\n",
       "      <td>Amorphus</td>\n",
       "      <td>Amorphus</td>\n",
       "      <td>Amorphus</td>\n",
       "      <td>Subtle</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Crites</th>\n",
       "      <td>Crites</td>\n",
       "      <td>Volpone</td>\n",
       "      <td>Mercury</td>\n",
       "      <td>Crites</td>\n",
       "      <td>Mercury</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volpone</th>\n",
       "      <td>Volpone</td>\n",
       "      <td>Volpone</td>\n",
       "      <td>Volpone</td>\n",
       "      <td>Volpone</td>\n",
       "      <td>Volpone</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mosca</th>\n",
       "      <td>Mosca</td>\n",
       "      <td>Mosca</td>\n",
       "      <td>Mosca</td>\n",
       "      <td>Mosca</td>\n",
       "      <td>Mosca</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Face</th>\n",
       "      <td>Face</td>\n",
       "      <td>Face</td>\n",
       "      <td>Face</td>\n",
       "      <td>Face</td>\n",
       "      <td>Face</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subtle</th>\n",
       "      <td>Subtle</td>\n",
       "      <td>Subtle</td>\n",
       "      <td>Subtle</td>\n",
       "      <td>Subtle</td>\n",
       "      <td>Subtle</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Part_1    Part_2    Part_3    Part_4   Part_5  Results\n",
       "Mercury    Mercury   Mercury   Mercury   Mercury  Mercury      1.0\n",
       "Amorphus  Amorphus  Amorphus  Amorphus  Amorphus   Subtle      0.8\n",
       "Crites      Crites   Volpone   Mercury    Crites  Mercury      0.4\n",
       "Volpone    Volpone   Volpone   Volpone   Volpone  Volpone      1.0\n",
       "Mosca        Mosca     Mosca     Mosca     Mosca    Mosca      1.0\n",
       "Face          Face      Face      Face      Face     Face      1.0\n",
       "Subtle      Subtle    Subtle    Subtle    Subtle   Subtle      1.0"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jonson_top_7_results = jonson_results.filter(items = jonson_top_7_characters, axis=0)\n",
    "jonson_top_7_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8857142857142858"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jonson_top_7_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21818181818181817"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jonson_bigram_results = pd.concat([jonson_bigram_df_1, jonson_bigram_df_2, jonson_bigram_df_3, jonson_bigram_df_4, jonson_bigram_df_5], axis =1)\n",
    "jonson_bigram_results = success_rate(jonson_bigram_results)\n",
    "jonson_bigram_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# William Shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characters (+1500 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_characters = {**macbeth_characters, **romeo_and_juliet_characters, **othello_characters, **hamlet_characters, **king_lear_characters}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shakespeare_characters.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_characters_split = corpus_split(shakespeare_characters, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_characters_partition = split_partitions(shakespeare_characters_split)\n",
    "shakespeare_characters_test = split_test(shakespeare_characters_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_part_1 = dict_word_tokenizer(shakespeare_characters_partition[0])\n",
    "shakespeare_part_2 = dict_word_tokenizer(shakespeare_characters_partition[1])\n",
    "shakespeare_part_3 = dict_word_tokenizer(shakespeare_characters_partition[2])\n",
    "shakespeare_part_4 = dict_word_tokenizer(shakespeare_characters_partition[3])\n",
    "shakespeare_part_5 = dict_word_tokenizer(shakespeare_characters_partition[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_test_1 = dict_word_tokenizer(shakespeare_characters_test[0])\n",
    "shakespeare_test_2 = dict_word_tokenizer(shakespeare_characters_test[1])\n",
    "shakespeare_test_3 = dict_word_tokenizer(shakespeare_characters_test[2])\n",
    "shakespeare_test_4 = dict_word_tokenizer(shakespeare_characters_test[3])\n",
    "shakespeare_test_5 = dict_word_tokenizer(shakespeare_characters_test[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_bigrams_part_1 = ngrams_tokenizer(dict_word_tokenizer(shakespeare_characters_partition[0]), 2)\n",
    "shakespeare_bigrams_part_2 = ngrams_tokenizer(dict_word_tokenizer(shakespeare_characters_partition[1]), 2)\n",
    "shakespeare_bigrams_part_3 = ngrams_tokenizer(dict_word_tokenizer(shakespeare_characters_partition[2]), 2)\n",
    "shakespeare_bigrams_part_4 = ngrams_tokenizer(dict_word_tokenizer(shakespeare_characters_partition[3]), 2)\n",
    "shakespeare_bigrams_part_5 = ngrams_tokenizer(dict_word_tokenizer(shakespeare_characters_partition[4]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_bigrams_test_1 = ngrams_tokenizer(dict_word_tokenizer(shakespeare_characters_test[0]), 2)\n",
    "shakespeare_bigrams_test_2 = ngrams_tokenizer(dict_word_tokenizer(shakespeare_characters_test[1]), 2)\n",
    "shakespeare_bigrams_test_3 = ngrams_tokenizer(dict_word_tokenizer(shakespeare_characters_test[2]), 2)\n",
    "shakespeare_bigrams_test_4 = ngrams_tokenizer(dict_word_tokenizer(shakespeare_characters_test[3]), 2)\n",
    "shakespeare_bigrams_test_5 = ngrams_tokenizer(dict_word_tokenizer(shakespeare_characters_test[4]), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 7 characters (+4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_top_7_characters = ['Macbeth', 'Romeo', 'Juliet', 'Othello', 'Iago', 'Hamlet', 'Lear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_top_7 = {}\n",
    "for key in shakespeare_characters:\n",
    "    if key in shakespeare_top_7_characters:\n",
    "        shakespeare_top_7[key] = shakespeare_characters[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_part_1_corpus = whole_corpus_generator(shakespeare_part_1)\n",
    "shakespeare_part_1_corpus_freq = list(nltk.FreqDist(shakespeare_part_1_corpus).most_common(50))\n",
    "shakespeare_part_1_features = features_generator(shakespeare_part_1_corpus_freq, shakespeare_part_1)\n",
    "df_shakespeare_1 = pd.DataFrame.from_dict(shakespeare_part_1_features, orient = 'index')\n",
    "shakespeare_zscores_1 = zscores(df_shakespeare_1)\n",
    "shakespeare_zscores_1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_test_1_features = features_generator(shakespeare_part_1_corpus_freq, shakespeare_test_1)\n",
    "df_shakespeare_test_1 = pd.DataFrame.from_dict(shakespeare_test_1_features, orient = 'index')\n",
    "shakespeare_zscores_test_1 = zscores(df_shakespeare_test_1)\n",
    "shakespeare_zscores_test_1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_1_deltas = get_deltas(shakespeare_characters, shakespeare_zscores_test_1, shakespeare_zscores_1)\n",
    "predictions_shakespeare_1 = shakespeare_1_deltas.idxmin()\n",
    "shakespeare_df_1 = pd.DataFrame(predictions_shakespeare_1, columns = ['Part_1'])\n",
    "shakespeare_df_1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_bigram_df_1 = model_predictions(shakespeare_bigrams_part_5, shakespeare_bigrams_test_5, 50, shakespeare_characters, 'Part_1')\n",
    "shakespeare_bigram_df_1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_part_2_corpus = whole_corpus_generator(shakespeare_part_2)\n",
    "shakespeare_part_2_corpus_freq = list(nltk.FreqDist(shakespeare_part_2_corpus).most_common(50))\n",
    "shakespeare_part_2_features = features_generator(shakespeare_part_2_corpus_freq, shakespeare_part_2)\n",
    "df_shakespeare_2 = pd.DataFrame.from_dict(shakespeare_part_2_features, orient = 'index')\n",
    "shakespeare_zscores_2 = zscores(df_shakespeare_2)\n",
    "shakespeare_zscores_2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_test_2_features = features_generator(shakespeare_part_2_corpus_freq, shakespeare_test_2)\n",
    "df_shakespeare_test_2 = pd.DataFrame.from_dict(shakespeare_test_2_features, orient = 'index')\n",
    "shakespeare_zscores_test_2 = zscores(df_shakespeare_test_2)\n",
    "shakespeare_zscores_test_2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_2_deltas = get_deltas(shakespeare_characters, shakespeare_zscores_test_2, shakespeare_zscores_2)\n",
    "predictions_shakespeare_2 = shakespeare_2_deltas.idxmin()\n",
    "shakespeare_df_2 = pd.DataFrame(predictions_shakespeare_2, columns = ['Part_2'])\n",
    "shakespeare_df_2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_bigram_df_2 = model_predictions(shakespeare_bigrams_part_2, shakespeare_bigrams_test_2, 50, shakespeare_characters, 'Part_2')\n",
    "shakespeare_bigram_df_2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_part_3_corpus = whole_corpus_generator(shakespeare_part_3)\n",
    "shakespeare_part_3_corpus_freq = list(nltk.FreqDist(shakespeare_part_3_corpus).most_common(50))\n",
    "shakespeare_part_3_features = features_generator(shakespeare_part_3_corpus_freq, shakespeare_part_3)\n",
    "df_shakespeare_3 = pd.DataFrame.from_dict(shakespeare_part_3_features, orient = 'index')\n",
    "shakespeare_zscores_3 = zscores(df_shakespeare_3)\n",
    "shakespeare_zscores_3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_test_3_features = features_generator(shakespeare_part_3_corpus_freq, shakespeare_test_3)\n",
    "df_shakespeare_test_3 = pd.DataFrame.from_dict(shakespeare_test_3_features, orient = 'index')\n",
    "shakespeare_zscores_test_3 = zscores(df_shakespeare_test_3)\n",
    "shakespeare_zscores_test_3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_3_deltas = get_deltas(shakespeare_characters, shakespeare_zscores_test_3, shakespeare_zscores_3)\n",
    "predictions_shakespeare_3 = shakespeare_3_deltas.idxmin()\n",
    "shakespeare_df_3 = pd.DataFrame(predictions_shakespeare_3, columns = ['Part_3'])\n",
    "shakespeare_df_3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_bigram_df_3 = model_predictions(shakespeare_bigrams_part_3, shakespeare_bigrams_test_3, 50, shakespeare_characters, 'Part_3')\n",
    "shakespeare_bigram_df_3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_part_4_corpus = whole_corpus_generator(shakespeare_part_4)\n",
    "shakespeare_part_4_corpus_freq = list(nltk.FreqDist(shakespeare_part_4_corpus).most_common(50))\n",
    "shakespeare_part_4_features = features_generator(shakespeare_part_4_corpus_freq, shakespeare_part_4)\n",
    "df_shakespeare_4 = pd.DataFrame.from_dict(shakespeare_part_4_features, orient = 'index')\n",
    "shakespeare_zscores_4 = zscores(df_shakespeare_4)\n",
    "shakespeare_zscores_4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_test_4_features = features_generator(shakespeare_part_4_corpus_freq, shakespeare_test_4)\n",
    "df_shakespeare_test_4 = pd.DataFrame.from_dict(shakespeare_test_4_features, orient = 'index')\n",
    "shakespeare_zscores_test_4 = zscores(df_shakespeare_test_4)\n",
    "shakespeare_zscores_test_4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_4_deltas = get_deltas(shakespeare_characters, shakespeare_zscores_test_4, shakespeare_zscores_4)\n",
    "predictions_shakespeare_4 = shakespeare_4_deltas.idxmin()\n",
    "shakespeare_df_4 = pd.DataFrame(predictions_shakespeare_4, columns = ['Part_4'])\n",
    "shakespeare_df_4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_bigram_df_4 = model_predictions(shakespeare_bigrams_part_4, shakespeare_bigrams_test_4, 50, shakespeare_characters, 'Part_4')\n",
    "shakespeare_bigram_df_4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_part_5_corpus = whole_corpus_generator(shakespeare_part_5)\n",
    "shakespeare_part_5_corpus_freq = list(nltk.FreqDist(shakespeare_part_5_corpus).most_common(50))\n",
    "shakespeare_part_5_features = features_generator(shakespeare_part_5_corpus_freq, shakespeare_part_5)\n",
    "df_shakespeare_5 = pd.DataFrame.from_dict(shakespeare_part_5_features, orient = 'index')\n",
    "shakespeare_zscores_5 = zscores(df_shakespeare_5)\n",
    "shakespeare_zscores_5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test z - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_test_5_features = features_generator(shakespeare_part_5_corpus_freq, shakespeare_test_5)\n",
    "df_shakespeare_test_5 = pd.DataFrame.from_dict(shakespeare_test_5_features, orient = 'index')\n",
    "shakespeare_zscores_test_5 = zscores(df_shakespeare_test_5)\n",
    "shakespeare_zscores_test_5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_5_deltas = get_deltas(shakespeare_characters, shakespeare_zscores_test_5, shakespeare_zscores_5)\n",
    "predictions_shakespeare_5 = shakespeare_5_deltas.idxmin()\n",
    "shakespeare_df_5 = pd.DataFrame(predictions_shakespeare_5, columns = ['Part_5'])\n",
    "shakespeare_df_5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_bigram_df_5 = model_predictions(shakespeare_bigrams_part_5, shakespeare_bigrams_test_5, 50, shakespeare_characters, 'Part_5')\n",
    "shakespeare_bigram_df_5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## William Shakespeare Characters Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_results = pd.concat([shakespeare_df_1, shakespeare_df_2, shakespeare_df_3, shakespeare_df_4, shakespeare_df_5], axis=1)\n",
    "shakespeare_results = success_rate(shakespeare_results)\n",
    "shakespeare_results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5142857142857143"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_top_7_results = shakespeare_results.filter(items = shakespeare_top_7_characters, axis=0)\n",
    "shakespeare_top_7_results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare_top_7_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21904761904761907"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare_bigram_results = pd.concat([shakespeare_bigram_df_1, shakespeare_bigram_df_2, shakespeare_bigram_df_3, shakespeare_bigram_df_4, shakespeare_bigram_df_5], axis =1)\n",
    "shakespeare_bigram_results = success_rate(shakespeare_bigram_results)\n",
    "shakespeare_bigram_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All top 7 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We take all the top +4000 words characters together, with the purpose of exploring the accuracy of each author top characters when presented among another large corpuses ones as noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_top_7 = {**wilde_top_7, **shaw_top_7, **jonson_top_7, **shakespeare_top_7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_top = corpus_split(all_top_7, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_top_partition = split_partitions(all_top)\n",
    "all_top_test = split_test(all_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_top_part_1 = dict_word_tokenizer(all_top_partition[0])\n",
    "all_top_part_2 = dict_word_tokenizer(all_top_partition[1])\n",
    "all_top_part_3 = dict_word_tokenizer(all_top_partition[2])\n",
    "all_top_part_4 = dict_word_tokenizer(all_top_partition[3])\n",
    "all_top_part_5 = dict_word_tokenizer(all_top_partition[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_top_test_1 = dict_word_tokenizer(all_top_test[0])\n",
    "all_top_test_2 = dict_word_tokenizer(all_top_test[1])\n",
    "all_top_test_3 = dict_word_tokenizer(all_top_test[2])\n",
    "all_top_test_4 = dict_word_tokenizer(all_top_test[3])\n",
    "all_top_test_5 = dict_word_tokenizer(all_top_test[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_top_bigrams_part_1 = ngrams_tokenizer(dict_word_tokenizer(all_top_partition[0]), 2)\n",
    "all_top_bigrams_part_2 = ngrams_tokenizer(dict_word_tokenizer(all_top_partition[1]), 2)\n",
    "all_top_bigrams_part_3 = ngrams_tokenizer(dict_word_tokenizer(all_top_partition[2]), 2)\n",
    "all_top_bigrams_part_4 = ngrams_tokenizer(dict_word_tokenizer(all_top_partition[3]), 2)\n",
    "all_top_bigrams_part_5 = ngrams_tokenizer(dict_word_tokenizer(all_top_partition[4]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_top_bigrams_test_1 = ngrams_tokenizer(dict_word_tokenizer(all_top_test[0]), 2)\n",
    "all_top_bigrams_test_2 = ngrams_tokenizer(dict_word_tokenizer(all_top_test[1]), 2)\n",
    "all_top_bigrams_test_3 = ngrams_tokenizer(dict_word_tokenizer(all_top_test[2]), 2)\n",
    "all_top_bigrams_test_4 = ngrams_tokenizer(dict_word_tokenizer(all_top_test[3]), 2)\n",
    "all_top_bigrams_test_5 = ngrams_tokenizer(dict_word_tokenizer(all_top_test[4]), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deltas Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Burrows' Delta Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_1 = model_predictions(all_top_part_1, all_top_test_1, 100, all_top_7, 'Part_1')\n",
    "all_df_2 = model_predictions(all_top_part_2, all_top_test_2, 100, all_top_7, 'Part_2')\n",
    "all_df_3 = model_predictions(all_top_part_3, all_top_test_3, 100, all_top_7, 'Part_3')\n",
    "all_df_4 = model_predictions(all_top_part_4, all_top_test_4, 100, all_top_7, 'Part_4')\n",
    "all_df_5 = model_predictions(all_top_part_5, all_top_test_5, 100, all_top_7, 'Part_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_top_df = pd.concat([all_df_1, all_df_2, all_df_3, all_df_4, all_df_5], axis = 1)\n",
    "all_top_df = success_rate(all_top_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.835714285714286"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_top_df['Results'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_in_all_results = all_top_df.filter(items = wilde_top_7_characters, axis=0)\n",
    "shaw_in_all_results = all_top_df.filter(items = shaw_top_7_characters, axis=0)\n",
    "jonson_in_all_results = all_top_df.filter(items = jonson_top_7_characters, axis=0)\n",
    "shakespeare_in_all_results = all_top_df.filter(items = shakespeare_top_7_characters, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7999999999999999"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wilde_in_all_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9428571428571428"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shaw_in_all_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jonson_in_all_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7428571428571429"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare_in_all_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Burrows' Delta Method with word bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bigrams_df_1 = model_predictions(all_top_bigrams_part_1, all_top_bigrams_test_1, 100, all_top_7, 'Part_1')\n",
    "all_bigrams_df_2 = model_predictions(all_top_bigrams_part_2, all_top_bigrams_test_2, 100, all_top_7, 'Part_2')\n",
    "all_bigrams_df_3 = model_predictions(all_top_bigrams_part_3, all_top_bigrams_test_3, 100, all_top_7, 'Part_3')\n",
    "all_bigrams_df_4 = model_predictions(all_top_bigrams_part_4, all_top_bigrams_test_4, 100, all_top_7, 'Part_4')\n",
    "all_bigrams_df_5 = model_predictions(all_top_bigrams_part_5, all_top_bigrams_test_5, 100, all_top_7, 'Part_5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_top_bigrams_df = pd.concat([all_bigrams_df_1, all_bigrams_df_2, all_bigrams_df_3, all_bigrams_df_4, all_bigrams_df_5], axis = 1)\n",
    "all_top_bigrams_df = success_rate(all_top_bigrams_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4714285714285714"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_top_bigrams_df['Results'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_bigrams_in_all_results = all_top_bigrams_df.filter(items = wilde_top_7_characters, axis=0)\n",
    "shaw_bigrams_in_all_results = all_top_bigrams_df.filter(items = shaw_top_7_characters, axis=0)\n",
    "jonson_bigrams_in_all_results = all_top_bigrams_df.filter(items = jonson_top_7_characters, axis=0)\n",
    "shakespeare_bigrams_in_all_results = all_top_bigrams_df.filter(items = shakespeare_top_7_characters, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45714285714285713"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wilde_bigrams_in_all_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6857142857142857"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shaw_bigrams_in_all_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42857142857142855"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jonson_bigrams_in_all_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31428571428571433"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare_bigrams_in_all_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# German Plays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Friedrich Schiller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characters (+1500 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "schiller_characters = {**kabale_und_liebe_characters, **die_verschwoerung_des_fiesco_zu_genua_characters, **die_ruber_characters, **die_jungfrau_von_orleans_characters}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(schiller_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "schiller_characters_split = corpus_split(schiller_characters, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "schiller_characters_partition = split_partitions(schiller_characters_split)\n",
    "schiller_characters_test = split_test(schiller_characters_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "schiller_part_1 = dict_word_tokenizer(schiller_characters_partition[0])\n",
    "schiller_part_2 = dict_word_tokenizer(schiller_characters_partition[1])\n",
    "schiller_part_3 = dict_word_tokenizer(schiller_characters_partition[2])\n",
    "schiller_part_4 = dict_word_tokenizer(schiller_characters_partition[3])\n",
    "schiller_part_5 = dict_word_tokenizer(schiller_characters_partition[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "schiller_test_1 = dict_word_tokenizer(schiller_characters_test[0])\n",
    "schiller_test_2 = dict_word_tokenizer(schiller_characters_test[1])\n",
    "schiller_test_3 = dict_word_tokenizer(schiller_characters_test[2])\n",
    "schiller_test_4 = dict_word_tokenizer(schiller_characters_test[3])\n",
    "schiller_test_5 = dict_word_tokenizer(schiller_characters_test[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "schiller_bigrams_part_1 = ngrams_tokenizer(dict_word_tokenizer(schiller_characters_partition[0]), 2)\n",
    "schiller_bigrams_part_2 = ngrams_tokenizer(dict_word_tokenizer(schiller_characters_partition[1]), 2)\n",
    "schiller_bigrams_part_3 = ngrams_tokenizer(dict_word_tokenizer(schiller_characters_partition[2]), 2)\n",
    "schiller_bigrams_part_4 = ngrams_tokenizer(dict_word_tokenizer(schiller_characters_partition[3]), 2)\n",
    "schiller_bigrams_part_5 = ngrams_tokenizer(dict_word_tokenizer(schiller_characters_partition[4]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "schiller_bigrams_test_1 = ngrams_tokenizer(dict_word_tokenizer(schiller_characters_test[0]), 2)\n",
    "schiller_bigrams_test_2 = ngrams_tokenizer(dict_word_tokenizer(schiller_characters_test[1]), 2)\n",
    "schiller_bigrams_test_3 = ngrams_tokenizer(dict_word_tokenizer(schiller_characters_test[2]), 2)\n",
    "schiller_bigrams_test_4 = ngrams_tokenizer(dict_word_tokenizer(schiller_characters_test[3]), 2)\n",
    "schiller_bigrams_test_5 = ngrams_tokenizer(dict_word_tokenizer(schiller_characters_test[4]), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 6 characters (+4000 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "schiller_top_6_characters = ['Ferdinand', 'Luise', 'Fiesco', 'Franz', 'Moor', 'Johanna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "schiller_top_6 = {}\n",
    "for key in schiller_characters:\n",
    "    if key in schiller_top_6_characters:\n",
    "        schiller_top_6[key] = schiller_characters[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Burrows' Delta Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Burrows' Delta Method\n",
    "schiller_df_1 = model_predictions(schiller_part_1, schiller_test_1, 50, schiller_characters, 'Part_1')\n",
    "schiller_df_2 = model_predictions(schiller_part_2, schiller_test_2, 50, schiller_characters, 'Part_2')\n",
    "schiller_df_3 = model_predictions(schiller_part_3, schiller_test_3, 50, schiller_characters, 'Part_3')\n",
    "schiller_df_4 = model_predictions(schiller_part_4, schiller_test_4, 50, schiller_characters, 'Part_4')\n",
    "schiller_df_5 = model_predictions(schiller_part_5, schiller_test_5, 50, schiller_characters, 'Part_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Burrows' Delta Method with word bi-grams\n",
    "schiller_bigrams_df_1 = model_predictions(schiller_bigrams_part_1, schiller_bigrams_test_1, 50, schiller_characters, 'Part_1')\n",
    "schiller_bigrams_df_2 = model_predictions(schiller_bigrams_part_2, schiller_bigrams_test_2, 50, schiller_characters, 'Part_2')\n",
    "schiller_bigrams_df_3 = model_predictions(schiller_bigrams_part_3, schiller_bigrams_test_3, 50, schiller_characters, 'Part_3')\n",
    "schiller_bigrams_df_4 = model_predictions(schiller_bigrams_part_4, schiller_bigrams_test_4, 50, schiller_characters, 'Part_4')\n",
    "schiller_bigrams_df_5 = model_predictions(schiller_bigrams_part_5, schiller_bigrams_test_5, 50, schiller_characters, 'Part_5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "schiller_results = pd.concat([schiller_df_1, schiller_df_2, schiller_df_3, schiller_df_4, schiller_df_5], axis = 1)\n",
    "schiller_results = success_rate(schiller_results)\n",
    "schiller_results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5411764705882354"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schiller_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "schiller_bigrams_results = pd.concat([schiller_bigrams_df_1, schiller_bigrams_df_2, schiller_bigrams_df_3, schiller_bigrams_df_4, schiller_bigrams_df_5], axis = 1)\n",
    "schiller_bigrams_results = success_rate(schiller_bigrams_results)\n",
    "schiller_bigrams_results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15294117647058827"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schiller_bigrams_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Part_1</th>\n",
       "      <th>Part_2</th>\n",
       "      <th>Part_3</th>\n",
       "      <th>Part_4</th>\n",
       "      <th>Part_5</th>\n",
       "      <th>Results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ferdinand</th>\n",
       "      <td>Lady Milford</td>\n",
       "      <td>Ferdinand</td>\n",
       "      <td>Ferdinand</td>\n",
       "      <td>Ferdinand</td>\n",
       "      <td>Ferdinand</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Luise</th>\n",
       "      <td>Luise</td>\n",
       "      <td>Luise</td>\n",
       "      <td>Lady Milford</td>\n",
       "      <td>Luise</td>\n",
       "      <td>Luise</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fiesco</th>\n",
       "      <td>Fiesco</td>\n",
       "      <td>Fiesco</td>\n",
       "      <td>Verrina</td>\n",
       "      <td>Fiesco</td>\n",
       "      <td>Fiesco</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Franz</th>\n",
       "      <td>Franz</td>\n",
       "      <td>Moor</td>\n",
       "      <td>Franz</td>\n",
       "      <td>Franz</td>\n",
       "      <td>Franz</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Moor</th>\n",
       "      <td>Moor</td>\n",
       "      <td>Moor</td>\n",
       "      <td>Franz</td>\n",
       "      <td>Moor</td>\n",
       "      <td>Moor</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Johanna</th>\n",
       "      <td>Johanna</td>\n",
       "      <td>Johanna</td>\n",
       "      <td>Johanna</td>\n",
       "      <td>Johanna</td>\n",
       "      <td>Johanna</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Part_1     Part_2        Part_3     Part_4     Part_5  \\\n",
       "Ferdinand  Lady Milford  Ferdinand     Ferdinand  Ferdinand  Ferdinand   \n",
       "Luise             Luise      Luise  Lady Milford      Luise      Luise   \n",
       "Fiesco           Fiesco     Fiesco       Verrina     Fiesco     Fiesco   \n",
       "Franz             Franz       Moor         Franz      Franz      Franz   \n",
       "Moor               Moor       Moor         Franz       Moor       Moor   \n",
       "Johanna         Johanna    Johanna       Johanna    Johanna    Johanna   \n",
       "\n",
       "           Results  \n",
       "Ferdinand      0.8  \n",
       "Luise          0.8  \n",
       "Fiesco         0.8  \n",
       "Franz          0.8  \n",
       "Moor           0.8  \n",
       "Johanna        1.0  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schiller_top_6_results = schiller_results.filter(items = schiller_top_6_characters, axis=0)\n",
    "schiller_top_6_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schiller_top_6_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Johann Wolfgang von Goethe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characters (+1500 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "goethe_characters = {**faust_1_characters, **faust2_characters, **egmont_characters, **iphigenie_auf_tauris_characters, **die_laune_des_verliebten_characters}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(goethe_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "goethe_characters_split = corpus_split(goethe_characters, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "goethe_characters_partition = split_partitions(goethe_characters_split)\n",
    "goethe_characters_test = split_test(goethe_characters_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "goethe_part_1 = dict_word_tokenizer(goethe_characters_partition[0])\n",
    "goethe_part_2 = dict_word_tokenizer(goethe_characters_partition[1])\n",
    "goethe_part_3 = dict_word_tokenizer(goethe_characters_partition[2])\n",
    "goethe_part_4 = dict_word_tokenizer(goethe_characters_partition[3])\n",
    "goethe_part_5 = dict_word_tokenizer(goethe_characters_partition[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "goethe_test_1 = dict_word_tokenizer(goethe_characters_test[0])\n",
    "goethe_test_2 = dict_word_tokenizer(goethe_characters_test[1])\n",
    "goethe_test_3 = dict_word_tokenizer(goethe_characters_test[2])\n",
    "goethe_test_4 = dict_word_tokenizer(goethe_characters_test[3])\n",
    "goethe_test_5 = dict_word_tokenizer(goethe_characters_test[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "goethe_bigrams_part_1 = ngrams_tokenizer(dict_word_tokenizer(goethe_characters_partition[0]), 2)\n",
    "goethe_bigrams_part_2 = ngrams_tokenizer(dict_word_tokenizer(goethe_characters_partition[1]), 2)\n",
    "goethe_bigrams_part_3 = ngrams_tokenizer(dict_word_tokenizer(goethe_characters_partition[2]), 2)\n",
    "goethe_bigrams_part_4 = ngrams_tokenizer(dict_word_tokenizer(goethe_characters_partition[3]), 2)\n",
    "goethe_bigrams_part_5 = ngrams_tokenizer(dict_word_tokenizer(goethe_characters_partition[4]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "goethe_bigrams_test_1 = ngrams_tokenizer(dict_word_tokenizer(goethe_characters_test[0]), 2)\n",
    "goethe_bigrams_test_2 = ngrams_tokenizer(dict_word_tokenizer(goethe_characters_test[1]), 2)\n",
    "goethe_bigrams_test_3 = ngrams_tokenizer(dict_word_tokenizer(goethe_characters_test[2]), 2)\n",
    "goethe_bigrams_test_4 = ngrams_tokenizer(dict_word_tokenizer(goethe_characters_test[3]), 2)\n",
    "goethe_bigrams_test_5 = ngrams_tokenizer(dict_word_tokenizer(goethe_characters_test[4]), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tp 6 characters (+4000 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "goethe_top_6_characters = ['Faust', 'Mephistopheles', 'Faust II', 'Mephistopheles II', 'Egmont', 'Iphigenie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "goethe_top_6 = {}\n",
    "for key in goethe_characters:\n",
    "    if key in goethe_top_6_characters:\n",
    "        goethe_top_6[key] = goethe_characters[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Burrows' Delta Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Burrow's Delta Method\n",
    "goethe_df_1 = model_predictions(goethe_part_1, goethe_test_1, 50, goethe_characters, 'Part_1')\n",
    "goethe_df_2 = model_predictions(goethe_part_2, goethe_test_2, 50, goethe_characters, 'Part_2')\n",
    "goethe_df_3 = model_predictions(goethe_part_3, goethe_test_3, 50, goethe_characters, 'Part_3')\n",
    "goethe_df_4 = model_predictions(goethe_part_4, goethe_test_4, 50, goethe_characters, 'Part_4')\n",
    "goethe_df_5 = model_predictions(goethe_part_5, goethe_test_5, 50, goethe_characters, 'Part_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Burrows' Delta Method with word bi-grams\n",
    "goethe_bigrams_df_1 = model_predictions(goethe_bigrams_part_1, goethe_bigrams_test_1, 50, goethe_characters, 'Part_1')\n",
    "goethe_bigrams_df_2 = model_predictions(goethe_bigrams_part_2, goethe_bigrams_test_2, 50, goethe_characters, 'Part_2')\n",
    "goethe_bigrams_df_3 = model_predictions(goethe_bigrams_part_3, goethe_bigrams_test_3, 50, goethe_characters, 'Part_3')\n",
    "goethe_bigrams_df_4 = model_predictions(goethe_bigrams_part_4, goethe_bigrams_test_4, 50, goethe_characters, 'Part_4')\n",
    "goethe_bigrams_df_5 = model_predictions(goethe_bigrams_part_5, goethe_bigrams_test_5, 50, goethe_characters, 'Part_5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "goethe_results = pd.concat([goethe_df_1, goethe_df_2, goethe_df_3, goethe_df_4, goethe_df_5], axis = 1)\n",
    "goethe_results = success_rate(goethe_results)\n",
    "goethe_results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goethe_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "goethe_bigrams_results = pd.concat([goethe_bigrams_df_1, goethe_bigrams_df_2, goethe_bigrams_df_3, goethe_bigrams_df_4, goethe_bigrams_df_5], axis = 1)\n",
    "goethe_bigrams_results = success_rate(goethe_bigrams_results)\n",
    "goethe_bigrams_results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14666666666666667"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goethe_bigrams_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Part_1</th>\n",
       "      <th>Part_2</th>\n",
       "      <th>Part_3</th>\n",
       "      <th>Part_4</th>\n",
       "      <th>Part_5</th>\n",
       "      <th>Results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Faust</th>\n",
       "      <td>Faust</td>\n",
       "      <td>Faust</td>\n",
       "      <td>Faust</td>\n",
       "      <td>Faust</td>\n",
       "      <td>Faust</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mephistopheles</th>\n",
       "      <td>Mephistopheles II</td>\n",
       "      <td>Mephistopheles</td>\n",
       "      <td>Mephistopheles</td>\n",
       "      <td>Mephistopheles</td>\n",
       "      <td>Mephistopheles</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Faust II</th>\n",
       "      <td>Faust</td>\n",
       "      <td>Faust II</td>\n",
       "      <td>Mephistopheles II</td>\n",
       "      <td>Mephistopheles II</td>\n",
       "      <td>Faust II</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mephistopheles II</th>\n",
       "      <td>Mephistopheles II</td>\n",
       "      <td>Mephistopheles II</td>\n",
       "      <td>Mephistopheles II</td>\n",
       "      <td>Mephistopheles II</td>\n",
       "      <td>Mephistopheles II</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Egmont</th>\n",
       "      <td>Egmont</td>\n",
       "      <td>Egmont</td>\n",
       "      <td>Egmont</td>\n",
       "      <td>Egmont</td>\n",
       "      <td>Egmont</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iphigenie</th>\n",
       "      <td>Faust</td>\n",
       "      <td>Egmont</td>\n",
       "      <td>Iphigenie</td>\n",
       "      <td>Iphigenie</td>\n",
       "      <td>Iphigenie</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Part_1             Part_2             Part_3  \\\n",
       "Faust                          Faust              Faust              Faust   \n",
       "Mephistopheles     Mephistopheles II     Mephistopheles     Mephistopheles   \n",
       "Faust II                       Faust           Faust II  Mephistopheles II   \n",
       "Mephistopheles II  Mephistopheles II  Mephistopheles II  Mephistopheles II   \n",
       "Egmont                        Egmont             Egmont             Egmont   \n",
       "Iphigenie                      Faust             Egmont          Iphigenie   \n",
       "\n",
       "                              Part_4             Part_5  Results  \n",
       "Faust                          Faust              Faust      1.0  \n",
       "Mephistopheles        Mephistopheles     Mephistopheles      0.8  \n",
       "Faust II           Mephistopheles II           Faust II      0.4  \n",
       "Mephistopheles II  Mephistopheles II  Mephistopheles II      1.0  \n",
       "Egmont                        Egmont             Egmont      1.0  \n",
       "Iphigenie                  Iphigenie          Iphigenie      0.6  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goethe_top_6_results = goethe_results.filter(items = goethe_top_6_characters, axis=0)\n",
    "goethe_top_6_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7999999999999999"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goethe_top_6_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Both top 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_top_6 = {**schiller_top_6, **goethe_top_6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_top = corpus_split(both_top_6, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_top_partition = split_partitions(both_top)\n",
    "both_top_test = split_test(both_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_top_part_1 = dict_word_tokenizer(both_top_partition[0])\n",
    "both_top_part_2 = dict_word_tokenizer(both_top_partition[1])\n",
    "both_top_part_3 = dict_word_tokenizer(both_top_partition[2])\n",
    "both_top_part_4 = dict_word_tokenizer(both_top_partition[3])\n",
    "both_top_part_5 = dict_word_tokenizer(both_top_partition[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_top_test_1 = dict_word_tokenizer(both_top_test[0])\n",
    "both_top_test_2 = dict_word_tokenizer(both_top_test[1])\n",
    "both_top_test_3 = dict_word_tokenizer(both_top_test[2])\n",
    "both_top_test_4 = dict_word_tokenizer(both_top_test[3])\n",
    "both_top_test_5 = dict_word_tokenizer(both_top_test[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_top_bigrams_part_1 = ngrams_tokenizer(dict_word_tokenizer(both_top_partition[0]), 2)\n",
    "both_top_bigrams_part_2 = ngrams_tokenizer(dict_word_tokenizer(both_top_partition[1]), 2)\n",
    "both_top_bigrams_part_3 = ngrams_tokenizer(dict_word_tokenizer(both_top_partition[2]), 2)\n",
    "both_top_bigrams_part_4 = ngrams_tokenizer(dict_word_tokenizer(both_top_partition[3]), 2)\n",
    "both_top_bigrams_part_5 = ngrams_tokenizer(dict_word_tokenizer(both_top_partition[4]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_top_bigrams_test_1 = ngrams_tokenizer(dict_word_tokenizer(both_top_test[0]), 2)\n",
    "both_top_bigrams_test_2 = ngrams_tokenizer(dict_word_tokenizer(both_top_test[1]), 2)\n",
    "both_top_bigrams_test_3 = ngrams_tokenizer(dict_word_tokenizer(both_top_test[2]), 2)\n",
    "both_top_bigrams_test_4 = ngrams_tokenizer(dict_word_tokenizer(both_top_test[3]), 2)\n",
    "both_top_bigrams_test_5 = ngrams_tokenizer(dict_word_tokenizer(both_top_test[4]), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Burrows Delta Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_df_1 = model_predictions(both_top_part_1, both_top_test_1, 100, both_top_6, 'Part_1')\n",
    "both_df_2 = model_predictions(both_top_part_2, both_top_test_2, 100, both_top_6, 'Part_2')\n",
    "both_df_3 = model_predictions(both_top_part_3, both_top_test_3, 100, both_top_6, 'Part_3')\n",
    "both_df_4 = model_predictions(both_top_part_4, both_top_test_4, 100, both_top_6, 'Part_4')\n",
    "both_df_5 = model_predictions(both_top_part_5, both_top_test_5, 100, both_top_6, 'Part_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_top_df = pd.concat([both_df_1, both_df_2, both_df_3, both_df_4, both_df_5], axis = 1)\n",
    "both_top_df = success_rate(both_top_df)\n",
    "both_top_df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666668"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both_top_df['Results'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "schiller_in_all_results = both_top_df.filter(items = schiller_top_6_characters, axis=0)\n",
    "goethe_in_all_results = both_top_df.filter(items = goethe_top_6_characters, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schiller_in_all_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goethe_in_all_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Burrows Delta Method Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_bigrams_df_1 = model_predictions(both_top_bigrams_part_1, both_top_bigrams_test_1, 100, both_top_6, 'Part_1')\n",
    "both_bigrams_df_2 = model_predictions(both_top_bigrams_part_2, both_top_bigrams_test_2, 100, both_top_6, 'Part_2')\n",
    "both_bigrams_df_3 = model_predictions(both_top_bigrams_part_3, both_top_bigrams_test_3, 100, both_top_6, 'Part_3')\n",
    "both_bigrams_df_4 = model_predictions(both_top_bigrams_part_4, both_top_bigrams_test_4, 100, both_top_6, 'Part_4')\n",
    "both_bigrams_df_5 = model_predictions(both_top_bigrams_part_5, both_top_bigrams_test_5, 100, both_top_6, 'Part_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_top_bigrams_df = pd.concat([both_bigrams_df_1, both_bigrams_df_2, both_bigrams_df_3, both_bigrams_df_4, both_bigrams_df_5], axis = 1)\n",
    "both_top_bigrams_df = success_rate(both_top_bigrams_df)\n",
    "both_top_bigrams_df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26666666666666666"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both_top_bigrams_df['Results'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "schiller_bigrams_in_all_results = both_top_bigrams_df.filter(items = schiller_top_6_characters, axis=0)\n",
    "goethe_bigrams_in_all_results = both_top_bigrams_df.filter(items = goethe_top_6_characters, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19999999999999998"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schiller_bigrams_in_all_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goethe_bigrams_in_all_results['Results'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
